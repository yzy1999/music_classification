{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1z3_hSLQc8AUSHVXuyARlsKvu5ehYn650",
      "authorship_tag": "ABX9TyOcrf8BJy/FOzpHJn3nSj59",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yzy1999/music_classification/blob/master/music_genre_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5Fd5fiWMoHV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d7a51861-5cd6-47f8-dee4-2ab98841e551"
      },
      "source": [
        "import random\n",
        "import math\n",
        "import librosa\n",
        "import librosa.feature\n",
        "import librosa.display\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import Input\n",
        "from keras import regularizers\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8otomFXwMoOx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "adf7a5d4-cff4-4675-eb56-5c38984e9fd2"
      },
      "source": [
        "#display\n",
        "def display_mel(song):\n",
        "    y, sr = librosa.load(song)\n",
        "    mel = librosa.feature.melspectrogram(y,  n_fft=1024,\n",
        "        hop_length=512, n_mels=256)\n",
        "    print(mel.shape)\n",
        "    mel = mel[:,:128]\n",
        "  #  print(mel.shape)\n",
        " #   print(mel)\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.specshow(mel, x_axis='time', y_axis='mel')\n",
        "    plt.colorbar()\n",
        "    plt.title(song)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "display_mel('/content/drive/My Drive/genres/blues/blues.00000.wav')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(256, 1293)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAosAAAEYCAYAAAA08LxnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2debwkVXn3f7+7zA4MMIhsMqgYBaJiEDUaRVFUNOJKMC6gGGJejRgTFTWvGpUEE+NufB2FCHFBAijEBSWyuaLs24iOLDLDwDDD7NvdnvePqgt9nqru03W7qru67+87n/7MPbWcPnX61KmnnvMsNDMIIYQQQgiRx1CvGyCEEEIIIeqLhEUhhBBCCNEUCYtCCCGEEKIpEhaFEEIIIURTJCwKIYQQQoimSFgUQgghhBBNkbAoRIeQvILkW1rsfx3JH3WzTb2A5BaSj+51O+oASSP52Cb7TiL50y6356skP9Zif9P2CiGEhEVRKST3Jbmywvo/TPJrJdaXK/iRfAbJn8+kTjP7upkd03nrHmrLD0kek167kTzV7T813f7hGdR9FMmpVPDbQnIlyfNIPjV2rpktMrM7in5nk3a8luQ3yqirW3QyRqqC5O0kH9frduRB8u9I3kdyE8mzSM5t2LeU5OUkt5H8Dcnnd+NcIUQ+EhZF1RwL4JJeN6IEXgLg+0VPIjlSZiNILgRwBIAr002/BfBGd9iJ6faZcq+ZLQKwC4CnA/gNgJ+QPLpJm0q9xpQZ9XcnlHAdXW9zK0g+BsCwmXUyFiqB5AsBnAbgaAAHAng0gH9qOOSbAK4HsCeADwA4n+ReXThXCJGDhEVRNccifYCSPIDkhSQfILmO5OfT7UMk/5Hk3STXkDyH5G7pvqWpluxEkn8guZbkB9J9LwLwfgB/kWrBbky370byTJKrSa4i+TGSw+m+k0j+lOQnSK4neSfJF6f7TgfwZwA+n9b3+SbX8YJUY7ExPYbTB6X1/4zkp0iuA/DhxmVHkl8k+YnGDiJ5Ecl3pX/vS/KCtI/uJPkO159HA/iZme1My78GsIDkoen5hwKYl26frv8Wkn/eUB5N+/HwVj+cJaw0sw8C+AqAjzfUYSTfRvJ3AH7XsO2xJJ+Wam6GG45/Bcmb0r+HSJ5G8vfpODiP5B4Nxw4BeAHSlwySb0zHxjqS/5fkXdPaolZ1tRo76f4Pkzyf5NdIbgJwUmTsPJbklenvvpbkt1yXPTRGpssk70iP/bf0ugIa2jjSsC3QbpN8M8nl6Xj9IckD0+1Mx9kaJlqym0ke1lC9F16XkLyU5Ob0Og7M/uq53x8sm5N8fFrPg0w0l8c37DuW5G3pd6wi+Q9534HkheZMM7vVzNYD+CiAk9I6HgfgKQA+ZGbbzewCADcDeFWV5+b0w5UkX5X+/cz0d3pJWj6a5A3p348heVk6/taS/DrJxem+95I839X7GZKfbdIvQtQSCYuiMkiOAng2gEvTB+53AdwNYCmA/QCcmx56Uvp5LpI3/UUAPh/WhmcB+CMkwtIHST7BzC4B8M8AvpUugT4pPfarACYAPBbA4QCOAdC4tPw0ALcDWALgXwGcSZJm9gEAPwHw9rS+t6fXsQ+AvQFcT3IJgAsB/GN6/u8BPNO19WkA7kjPOd3t+yYS4ZZp3bun7Ts3FSb+B8CNaf8cDeCdTLQh0xwL4Huuzv/Cw9rFE9NyI+cAeL2rY7WZXY/2uRDAU5hoNqd5OZJrPaTxQDO7GsBWAM9r2PyXAKaXlf82Pfc5APYFsB7AFxqOPRLAHWa2luQhAP4DwOsA7ANgNyR9gzbrAnLGTsO+4wCcD2AxgK+j9dj5KIAfAdgdwP4APjddSeMYaaj7FUi0wE9Jv+fNKAjJ45C8EL0SwF5Ixuc3093HILm/HoekX44HsK7hdD9WXpdewxIAN6TXW7Q9CwFciuS3fASAEwD8R/o7AcCZAP7azHYBcBiAy5pUdSiScT7NjQD2Jrlnuu8OM9vs9h9a8bmeKwEclf79HCT39LMbytPafQL4FyTj7wkADgDw4XTfuUheGnYBgHQePB4P3wtC9AUSFkWVPBvAjenEfSSSyfTdZrbVzHaY2bS24nUAPmlmd5jZFgDvA3ACw2XBf0o1BTcimeCfhBxI7o3kIfnO9HvWAPgUkofaNHeb2ZfNbBLA2UiEkL1bXMexAC6xJJH6sQBuNbPzzWwcwKcB3OeOv9fMPmdmE2a23e37CQBDosEEgFcD+IWZ3QvgqQD2MrOPmNlYav/3Zdd2r70CgK8BeG0qnJ+Qlv3+Y0numpbfgKxAGeNeJA/FxQ3b/sXMHsy5RiARaF4LAOmD8lg8LOS8FcAHUq3lTiQP1lc3/N6NGrFXA/gfM/upmY0B+CCS/kObdQGtx84vzOw7ZjYFYFe0HjvjSJYu93XjFwjHyDQfT/vnD0jGyWtz+inGW5H083Izm0DycvTkVCs4jsRU4PEAmB6zGgBILkAynq5oqOt7ZnZV2k8fAPAMkgcUbM9LAdxlZv+Zju/rAVwA4DXp/nEAh5Dc1czWm9l1TepZBGBjQ3n6711y9k3v36Xicz1XIhEKgWQu+5eG8kPCopmtMLNLzWynmT0A4JPTx5nZ3QCuQ/LiACQvUNvM7Jc53ydEbZGwKKqkUbA5AImQNpFz3L5INI7T3A1gBKEA1yiQbUMy6edxIIBRAKtJbiC5AcCXkGhBMnWZ2bb0z2b1+evYF8A9DedbYznFlx8iPf5cPCw4/CUe1vAcCGDf6XanbX8/0n4g+ccANprZPa7OPwBYgUSQ+F3O/nsB/AzAq9LlsRejuFZpPyRC2oZ2rhOJ5uSVTJwHXgnguvTBOX2d3264xuUAJvHw792qv7ch1J7F6gJaj53Ga4iNnfcgEZh/RfJWko2awjwhvrHuu9NrKcqBAD7T0J4H0zbsZ2aXIdHAfwHAGpLLGl4Ijgbw8wZzhaA96UvZgzNo04EAnubG6OsAPDLd/yokfXF3uoz7jCb1bEEinE8z/ffmnH3T+6e1hVWd6/kFgMelL6BPRqKhPyBdXTgSwFVA8oJK8tx02X0TkpezJQ31fAPh/S6toug7JCyKKml8gN4D4FHMdyK4F8lDaJpHIVkKvL+N7zBXvgfATgBLzGxx+tnVzA7NOTdaX6qtew6SpTcAWI1E8J3ez8ZykzZ5volE+3UgkmXcCxrafmdDuxeb2S5mdmy6P08gmeYcAH+f/p/H2UiWol+DRJu2KtJGzyuQCHxbG7Y1vU4zuw2JgPRiZB+Q9wB4sbvOeWa2iuQjkWh6pzVSq5Es+QIASM5H4rgQravN62q8hpZjx8zuM7O/MrN9Afw1kuXXx+aMkWkax8WjkIxzz3R/LmjY9siGv+9BsqzbeH3zzeznaZs+a2Z/gsQU4HEA3p2elzdWGsftIgB7tGhTq/Zc6dqzyMz+Jm3Pr83sOCQC9ncAnJdTPwDcilDD+yQA95vZunTfo6eXbhv231rxuQHpi8m1AE4FcEuq2f45gHcB+L2ZrU0P/Wck4+iPzWxXJPcZG6r6bwBHkdwfyX0kYVH0HRIWRSWQPAjAXDNbnm76FZIH/xkkF5KcR3La1u+bAP6O5EHpQ2zaDjFPC+m5H8DS1N4P6TLcjwD8O8ldmThAPIbkc1rWEtbXGCvwWQBuMrNNafl7AA4l+cpU8H0HwodplHTpbi0Sp5Efmtm0tu5XADanRvHzSQ6TPIwPh63Js1ec5ltIbNiaPZy/g8R27lQ0FygDmLAfyQ8hsdt7fzvnNfCN9PuejeSBOc3/A3A6H3bU2Cu1zQMS4bJxOfd8AH9O8k9JzkGyzMw26ypEbOyQfE36wAcS20gDMIXsGJnm3SR3T5d6T0XyG/nvfADAKgCvT3/vNwN4jLu+9/FhB6bdSL4m/fupTJyJRpEIeDvS9gBJP/qxcizJZ6X9+FEAv/Ra6JQbkGiFFzCJvXhyw77vItG2vYGJo9Ro2o4nkJzDJKbobqmJxqaG9njOAXAyyUNSbfc/IrEXhSXe2zcA+FA6T7wCwBPx8EtVJec24UoAb8fD9olXuDKQLGFvAbCR5H54WGBH2qYH0vP+E8nL4HII0WdIWBRVEXhiWmIf+OdIHAf+AGAlgL9Id5+FxIbuKgB3Inno/W2b3zMthKwjOa2NeiOAOQBuQ/JQPx+JtqodPoNE67eeiceiv461SLRzZyBZDj0YyRJvUb4B4Plo0DKkffRSJEted+JhgXK39MF2CBLNRobUJu9/m9gPIt1+AYCDkDirtGJfkluQPAB/DeCPARxlZkUDi38TicbtsgYtDJD08cUAfkRyM4BfItGwAtn+vhXJWDgXycvGFgBrkGgAY3XNhFZj56kArk775mIAp1piV9osZM5FSDRTNyAR3M5s8p1/hUTAWIfEAeOh39jMvo3EC/3cdInzFiSCIJAsoX45befd6fn/xsQjektqntDINwB8CMny858gdHpq5FMAxpC8OJ2NBpMFS+yPj0Fix3kvkiX+jwOYjlX4BgB3pW19K5IlapB8FJMIA49K67kEiXPZ5Ujmg7vTtk1zAhLnoPVI7rVXp0JXpeem5gWva6jrSiTC4FVNykASeucpSOwfv4f8+ytzvwvRTzC0xxaiHEh+H8Dnzaw2cedmAsnbkDxsbutxO45P23F89ODmdXwQwOPMrJmQ0FNSTe19AB6do6WbPmYRErvJg83szm62rxl1GSPTkHwPkqX09/S6LUKIwUCaRVEVVyB5e+9b0uW6c2oiBGxAovGZEUxiD54MYFlpLSqfPQD8Xy8okvzzdEl0IYBPIImbd1cP2pehZmNkmruQLHkKIUQpSLMoxIBD8q+QhG75LzN7a6/bUxSSX0ESQocArgHwf8zs9t62SgghZg8SFoUQQgghRFO0DC2EEEIIIZqSF/NuICAplakQQggh8lhrZnv1sgEvfOGRtm6dTziUz7XX/vaHZvaiipvUlIEVFhMG/PKEEEIIMQMm7o4fUy3r1m3E1b/6UlvHjgw/d0n8qOqQNCWEEEII0W0MwFSzuPX1QsKiEEIIIUTXMWCinURlvUfCohBCCCFEtzEAfRKRRsKiEEIIIUTXsb5ZhlboHCGEEEKIXjA11d6nDUgOk7ye5HfT8kEkrya5guS30oxTIDk3La9I9y+N1S1hUQghhBCi20w7uJQkLAI4FcDyhvLHAXzKzB4LYD2SlK9I/1+fbv9UelxLJCwKIYQQQnQdK01YJLk/gJcA+EpaJoDnATg/PeRsAC9P/z4uLSPdf3R6fFNksyiEEEII0W3MwMnSvKE/DeA9AHZJy3sC2GBm01+wEsB+6d/7AbgnaYJNkNyYHr+2WeXSLAohhBBC9IL2NYtLSF7T8DllugqSLwWwxsyuraqZ0iwKIYQQQnQbAzDVduictWZ2RJN9zwTwMpLHApgHYFcAnwGwmORIql3cH8Cq9PhVAA4AsJLkCIDdAKxr9eXSLAohhBBCdJ1ybBbN7H1mtr+ZLQVwAoDLzOx1AC4H8Or0sBMBXJT+fXFaRrr/MrPWAR8lLAohhBBCdJvyvaE97wXwLpIrkNgknpluPxPAnun2dwE4LVZRpcvQJP8OwFuQdMnNAN6Ult8J4DEA9jKztemxuwM4K92+A8CbzewWkgcAOAfA3mk9y8zsM1W2WwghhBCicqzcoNxmdgWAK9K/7wBwZM4xOwC8pki9lWkWSe4H4B0AjjCzwwAMI1GP/gzA8wHc7U55P4AbzOyJAN6IZL0dACYA/L2ZHQLg6QDeRvKQqtothBBCCFE5ZsDEZHufHlP1MvQIgPmpAeUCAPea2fVmdlfOsYcAuAwAzOw3AJaS3NvMVpvZden2zUgCTu6Xc74QQgghRJ9QXpzFqqlMWDSzVQA+AeAPAFYD2GhmP2pxyo0AXgkAJI8EcCAS752HSFPSHA7g6vJbLIQQQgjRRWa7sJjaIB4H4CAA+wJYSPL1LU45A4mb9w0A/hbA9QAe0r2SXATgAgDvNLNNTb7zlOkYRCVdhhBCCCFE+RjAqam2Pr2mSgeX5wO408weAACSFwL4UwBfyzs4FQDflB5LAHcCuCMtjyIRFL9uZhc2+0IzWwZgWXpO28GLhBBCCCG6iyV2i31AlcLiHwA8neQCANsBHA2gqcaP5GIA28xsDInH9FVmtikVHM8EsNzMPllhe4UQQgghukcNtIbtUKXN4tVIElRfhyRszhCAZSTfQXIlEnvEm0h+JT3lCQBuIXk7gBcDODXd/kwAbwDwPJI3pJ9jq2q3EEIIIUTl9JE3dKVxFs3sQwA+5DZ/Nv34Y38B4HE5238KgJU0UAghhBCiF0wH5e4DlBtaCCGEEKIXSFgUQgghhBD5yMFFCCGEEEI0Q8vQQgghhBCiJVPSLAohhBBCiDzMgImJXreiLSQsCiGEEEL0AmkWhRBCCCFEPgaYbBaFEEIIIUQeBmkWhRBCCCFEC+QNLYQQQgghcukjzWJluaGFEEIIIUQzyssNTXIeyV+RvJHkrST/Kd3+VZJ3krwh/Tw53U6SnyW5guRNJJ/Sqn5pFoUQQgghuo2hTAeXnQCeZ2ZbSI4C+CnJH6T73m1m57vjXwzg4PTzNABfTP/PRcKiEEIIIUTXsdKWoc3MAGxJi6Ppp1XlxwE4Jz3vlyQXk9zHzFbnHaxlaCGEEEKIXjBl7X2AJSSvafic4qsiOUzyBgBrAFxqZlenu05Pl5o/RXJuum0/APc0nL4y3ZaLNItCCCGEEN2mWG7otWZ2RMvqzCYBPJnkYgDfJnkYgPcBuA/AHADLALwXwEeKNlWaRSGEEEKIXtC+ZrFtzGwDgMsBvMjMVlvCTgD/CeDI9LBVAA5oOG3/dFsuEhaFEEIIIbqNleoNvVeqUQTJ+QBeAOA3JPdJtxHAywHckp5yMYA3pl7RTwewsZm9IqBlaCGEEEKI3mClxVncB8DZJIeRKALPM7PvkryM5F4ACOAGAG9Nj/8+gGMBrACwDcCbWlUuYVEIIYQQotuUGJTbzG4CcHjO9uc1Od4AvK3d+iUsCiGEEEJ0nfJC51SNhEUhhBBCiF6g3NBCCCGEECKXPsoNLWFRCCGEEKLbmAET0iwKIYQQQogmmDSLQgghhBCiKeWFzqkUCYtCCCGEEN1GNotCCCGEEKIlEhaFEEIIIUQupjiLQgghhBCiBSZvaCGEEEIIkYtsFoUQQgghREskLAohhBBCiFz6yGZxqOovIDlM8nqS303LB5G8muQKkt8iOSfdfiDJH5O8ieQVJPdvqONRJH9EcjnJ20gurbrdQgghhBCVYtbep8dULiwCOBXA8obyxwF8ysweC2A9gJPT7Z8AcI6ZPRHARwD8S8M55wD4NzN7AoAjAaypvNVCCCGEEBVhAGyqvU+vqVRYTLWDLwHwlbRMAM8DcH56yNkAXp7+fQiAy9K/LwdwXHrOIQBGzOxSADCzLWa2rcp2CyGEEEJUiiHJDd3Op8dUrVn8NID3AJi+0j0BbDCzibS8EsB+6d83Anhl+vcrAOxCck8AjwOwgeSF6XL2v5EczvsykqeQvIbkNVVcjBBCCCFEWcx6zSLJlwJYY2bXtnnKPwB4DsnrATwHwCoAk0iccP4s3f9UAI8GcFJeBWa2zMyOMLMjOmy+EEIIIUR1TIfOaecTgeQ8kr8ieSPJW0n+U7q9mZ/I3LS8It2/tFX9VWoWnwngZSTvAnAukuXnzwBYTHLaC3t/JEIhzOxeM3ulmR0O4APptg1ItI83mNkdqUbyOwCeUmG7hRBCCCGqZ6rNT5ydAJ5nZk8C8GQALyL5dDT3EzkZwPp0+6fS45pSmbBoZu8zs/3NbCmAEwBcZmavQ2KP+Or0sBMBXAQAJJeQnG7P+wCclf79ayQC5l5p+XkAbquq3UIIIYQQlWMGm2rvE6/KzMy2pMXR9GNo7idyXFpGuv/o1K8kl254Q3veC+BdJFcgsWE8M91+FIDbSf4WwN4ATgcAM5tEsgT9Y5I3AyCAL3e70UIIIYQQpdK+ZnHJtE9G+jnFV5WGKrwBScSYSwH8Hs39RPYDcA8ApPs3IpHJculKUG4zuwLAFenfdyAJf+OPOR8PS79+36UAnlhdC4UQQgghuogBNtl2DMW1MX+MVLn2ZJKLAXwbwOM7bOFD9EKzKIQQQgghyrNZfIjU3+NyAM9AEz+R9P8DACDdvxuAdc3qlLAohBBCCNEDygqdQ3KvVKMIkvMBvABJQpRcPxEAF6dlpPsvM2ueKka5oYUQQgghuo2hsNawBfsAODuNQz0E4Dwz+y7J2wCcS/JjAK7Hw34iZwL4r9R/5EEkjshNkbAohBBCCNFlptP9lVKX2U0ADs/Z3sxPZAeA17Rbv4RFIYQQQohuU65msVIkLAohhBBC9ICpyV63oD0kLAohhBBCdBtpFoUQQgghRCvKslmsGgmLQgghhBA9oHmwmnohYVEIIYQQotsYgKmm6ZhrhYRFIYQQQoguU2bonKqRsCiECCDCN11Dn6yTCCFEP2HE1KQ0i0KIPkTCoRBCdAdpFoUQQgghRC4GwEyaRSGEEEIIkYcBJgcXIYQQQgjRDIXOEUIIIYQQTdEytBBCCCGEyMUM8oYWQgghhBDNoDSLQgghhBCiOVNycBFCCCGEELlY/zi4DPW6AUIIIYQQs43pOIvtfGKQPIDk5SRvI3kryVPT7R8muYrkDenn2IZz3kdyBcnbSb6wVf3SLAohhBBC9IASbRYnAPy9mV1HchcA15K8NN33KTP7ROPBJA8BcAKAQwHsC+B/ST7OzCbzKpewKIQQA4jP8e1RWkcheowRk5PlLPCa2WoAq9O/N5NcDmC/FqccB+BcM9sJ4E6SKwAcCeAXeQdrGVoIIQYRMvwIIWpFsgzd3gfAEpLXNHxOaVYvyaUADgdwdbrp7SRvInkWyd3TbfsBuKfhtJVoIVxKszhLydM6SNMgxOBgNtXrJlTOEEeD8pSN96glQsyMqfaXodea2RGxg0guAnABgHea2SaSXwTwUSSy6UcB/DuANxdtp4TFmkAOubKbBKd2lvp9EgyFEP2OIde8Soi+ocw4i0wEhwsAfN3MLkzqt/sb9n8ZwHfT4ioABzScvn+6LRctQw8o5FDwEUKIQcNsKvh0CvP+ubnU7xdiphgSzWI7nxgkCeBMAMvN7JMN2/dpOOwVAG5J/74YwAkk55I8CMDBAH7VrH5pFmuCn+gSm9OOKuzsfDGw+AectMxCNCcmhOp+EjPGStUsPhPAGwDcTPKGdNv7AbyW5JOTb8NdAP4aAMzsVpLnAbgNiSf125p5QgMSFgcWTViiGRobQuQzk3tD95OYOcRkScKimf0UyFV1f7/FOacDOL2d+iUsCiGEEEJ0mell6H6gMmO2FtHE9yB5Kcnfpf/v7s57KskJkq9u2PavaR3LSX42XZsXQgghhOhbysrgUjVVej5MRxM/BMDTAbwtjRh+GoAfm9nBAH6clgEAJIcBfBzAjxq2/SmStfgnAjgMwFMBPKfCdosBRsbpQggh6sKUtffpNZUJi2a22syuS//eDGA6mvhxAM5ODzsbwMsbTvtbJG7faxqrAjAPwBwAcwGMArgffU6u152EmMox908IIYToBWb9o1nsis2iiya+d5qWBgDuA7B3esx+SNy6n4tEewgAMLNfkLwcSRobAvi8mS1v8j2nAGga1VwIIUR9kCexmO1M9YlyqHJhMSea+EP7zMxITs8OnwbwXjObajyG5GMBPAFJwEgAuJTkn5nZT/x3mdkyAMvS82o962hSFGJ2I0GpftfsM8IAgNlEy3Pqdg2ifzAQk1MSFnOjiQO4n+Q+ZrY6DRY5veR8BIBzU0FxCYBjSU4gCRT5SzPbktb5AwDPAJARFoUQWSSU1JN++x0GcRwNDc0Pyja1I3NM7DoHsV9E9+gXzWKV3tC50cSRRA0/Mf37RAAXAYCZHWRmS81sKYDzAfwfM/sOgD8AeA7JkVT4fA4S+8eBImbDGM0qoCwDIiXz2zP8aGyIdvDjZCDtfW0i+PhrbOc6/fG6v0QRErvF+KfXVKlZbBZN/AwA55E8GcDdAI6P1HM+gOcBuBmJs8slZvY/1TS5dxSefN3o0dutmMb/9pkHlo88VYeZSHSd2JwxiHOIv+YpG+9RS4TorziLlQmLLaKJA8DRkXNPavh7Eml6mm4RfbgingKq2wzixC7KoW5jVdSDQZgz6vgiNAj9KrqH9Yn2WRlccsjc7NK8CCFE7Yhp0SW4iVpTkxiK7SBhUQghxGCgF3vRRxiISasyN0p5tNVKkn+WZldp3PaUapokhGhGO85Pon7od+oOA+mEIwaafsng0q5m8YcAfk3yNWY2HermKwAkMIraQIbDOe+B7GOm9d0DJXxnS3E2idKu1I6+G2czJLYMTA65chjXMDFRbyy3jnE4GxgamhuU69BHZS/3Z21P3Tznr3mA7qdBs1m8HcC/AbiS5Mlm9nM0d14Roje4CWVqgCaUafTwFHUm9hD3zlZmO6tszkAwNVW/PipbWMv6CcyOeS7xhu51K9qjXWHRzOy7JG8H8C2SZwED+CQWfc0gvW0KUTfkPCJE+fRL6Jx2LSsJAGb2OwDPTj9PrKpRQggh6oXsAYUoH2vz02vaEhbN7PCGv7eY2fEAHl1Zq4QQQnSEHGqEqDdmwISxrU8MkgeQvJzkbSRvJXlqun0PkpeS/F36/+7pdpL8LMkVJG+KOS23XIYm+Tm0FmrfEb0CIYQQ3acGAaqFEK2x8pahJwD8vZldR3IXANeSvBTASQB+bGZnkDwNwGkA3gvgxQAOTj9PA/DF9P9cYjaL1zT8/U8APjTTqxBC9AbZms1OlLlHtEvUi11zSCUYMrEsZl6X2WoAq9O/N5NcDmA/AMcBOCo97GwAVyARFo8DcI6ZGYBfklxMcp+0ngwthUUzO3v6b5LvbCwLIfoDTexCiFZEvdg1h1RGFd7QJJcCOBzA1QD2bhAA7wOwd/r3fgDuaThtZbqtuLDo0GgRQgghhCgFFomzuIRk42rvMjNblqmRXATgAgDvNLNNbDBHMTMjOSNZTun+hBBCCCG6TME4i2vN7IhWBzCJcn8BgK+b2YXp5vunl5dJ7gNgOrHKKgAHNJy+f7otl5be0CQ3k9xEchOAJ07/Pb09cs7v7xAAACAASURBVGFCCCFmEdFklBxyn5GWHyEGnUljW58YTFSIZwJYbmafbNh1MYAT079PBHBRw/Y3pl7RTwewsZm9IhC3Wdwl2kIBoPr0R7IZEWXhU6555Bgh2iE3HE8kTVvROjXriUHGys37/EwAbwBwM8kb0m3vB3AGgPNIngzgbgDHp/u+D+BYACsAbAPwplaV69WtJCpPfyREm0RfNHwIFR9iRYgc/EtG3ksFYzl8M+F7fB0ai2J2UVZuaDP7KZrfQEfnHG8A3tZu/RIWhehzigZcjj3A2xEKxODjxwERag0tJ+hHpy+53X5JrmKsa1VIFGHQckML0XfkCVGDOHEXvaZMvyh480CS/Z2HI/u94BRqCadsvLS2tUvlgpfGei2ZLQJ3mXEWq0bCohhYBnWCKR33wFS/9Sexl4ChoQVBeWpqW3i8EwbroFEueyzOFiGk35jN6ShLzOBSKRIWhZhl6AFZT2KORxkiWjEv7E1NbWm5X5SD7q84EtoTDMBEn1y6hEUxq4kt1cUWCfTAFaVRcEm06ANWY3X2CiV1Q7/Dw/RLT0hYFLOaTA7UgqE+ROfMFtvSGLPxmoWYzSRBubUMLUTfoQd298nrc3lkCyFmA/3yxJGwKIQQQoieMWttGMsNyl0pEhbFrCaTUiwWVFh0BWkShRhcigqHgypMKnSOEH2CDydiU9vDcg9iy5VN1MtWoXOEEKIHtJf3uQ5IWOwRcS9cT+T9Qw/8thgZWRyUF8zdOyhv23l/UJ6a2FB5mzqlaPDlWHBl30dey+cF6lh9sj8U0+jFZXYyNDQ3KPuA77CJlucP8jjol7jwEhZrAqPCYkFhchbffNPkPZi8pnDztt+7I/pQkHHBl/1YmjcnFIi371zpjg8n8tHhRUF5fDKMz4ehOWF5MqZ99b9DH/axaItodqDMWMhUEBKZx8omNzh0JMPRIM6lZb/gDcIKTRVoGVpEyUwwU2OdVZiZlCPfNwvIm+AmJ7f2oCXVMjQ0PygfvstfBuUnzg+FxR9t/2lQXrd9RVDeOb4uKE85TeLw8MKW7ckK6eHvUIX90SDYNNX9GuKCILJqkozapLVNcOHA5B1SfIUHGYGWbnwPhOa8ZHWX75NM3nH/u88CgXwaObiIQnR8M/SLLlt0RN7DdM7I7kF5xXgoDN62IxSQxyY2BeWJgkvtMYE782DoQiqvQXiY1O0aYr9bFUJR1wUtJxzaDDSZg5iqrnC++YgmMttHrV8KDN3VKPeSet31zZGwKESfMzax3pVDYTCzBNTlpb26CUGiPWbF71ZCEP6MdrTmGuIqIEfDDRZbKfNmU4PfR3kkQbl73Yr2qExYJHkWgJcCWGNmh6Xb9gDwLQBLAdwF4HgzW0/ydQDei0TBvxnA35jZjQ11DQO4BsAqM3tpVW0Wg0+/T+SzZWl90KgiS01mKc/ZNWfrb/2A7rd7oR18vw+P7BaUi2rV2/vS8HcYKuhg1g/4sTfqVjcmp3aE5Un/AjsAS/VlYMBkn9x2VWoWvwrg8wDOadh2GoAfm9kZJE9Ly+8FcCeA56SC44sBLAPwtIbzTgWwHMCuFbZXDBiZGIoAYvZzVefnFbOTSsaJF/boPUwHXxj0ZM00wrK3v60EH6u1HTvIPsMLe+POztm8baqEw1ykWQRgZleRXOo2HwfgqPTvswFcAeC9ZvbzhmN+CWD/6QLJ/QG8BMDpAN5VTWtFPxLTEvoYikA27AsizhiZ8yO5pDvWFsXsxGbBA38QKUOz2JaDSQf1DwJ+OdQ7f01N7ay8DZl5yLVpJnaRdcdf02wcezOlX3qq2zaLe5vZ6vTv+wDsnXPMyQB+0FD+NID3ANglVjnJUwCc0mkjRZ/gQ8a4Zbi5o2G8QADYvtOFgYlqErv7RhyzfxL9SRkPz0wds9TOqxU+fl8dlj/NLckOIkXnLQmTD1OWZrGJ6d+HAfwVgAfSw95vZt9P970Pibw1CeAdZvbDVvX3zMHFzIxk0E0kn4uk8c9Ky9MXfi3Jo9qocxmSJWz4usUg0tqj7qxDXpfZ9trrzgg3ZOa0moXBiMR4E0I8TB21drNRMJqN1zwTDICVpxD4KrKmfwDwKTP7ROMGkocAOAHAoQD2BfC/JB9nmWjpD9NtYfF+kvuY2WqS+wBYM72D5BMBfAXAi81s2gDimQBeRvJYAPMA7Erya2b2+i63W9QRN67nzt0nKO82mn1w7Lbw8UF5q8vYMjFZzOC9qOav6CTac2FVCCFEZZSlWWxi+teM4wCca2Y7AdxJcgWAIwH8otkJ3Y2AClwM4MT07xMBXAQAJB8F4EIAbzCz304fbGbvM7P9zWwpEin4MgmKohlmU8Fnwpj5VN6GyD8hhBACSDSLk9beB8ASktc0fNo1uXs7yZtInkVy2m19PwD3NByzMt3WlCpD53wTiTPLEpIrAXwIwBkAziN5MoC7ARyfHv5BAHsC+A8my24TZnZEVW0Tg4EXvny4hn++3dknAnjyyAuD8lXbvxSUs+FHijm8dIr34Pap+7phoO/p93BDQghRS6yQZnHtDOSiLwL4aPJN+CiAfwfw5oJ1AKjWG/q1TXYdnXPsWwC8JVLfFUi8p4XIZXJiY1D+9dZvZI4ZcsKYNz7vuSCU8a7u/jK0nGqEEKI7VPnMMbOH7KxIfhnAd9PiKgAHNBy6f7qtKcrgIgYGf9PlBdzNhLHotXAYwWs68wItlB2uJ6PdHJoTlL12M+ZU4OuroxOCEEJ0m6rjLE77iKTFVwC4Jf37YgDfIPlJJA4uBwP4Vau6JCyKWUW/ZU+YSXujmkEfcsin6vKBjCe3ufNbBxnOCp+DH2dOCCFmQlmyYhPTv6NIPjn9mrsA/DUAmNmtJM8DcBuACQBva+UJDUhYFEJkiC19F10al0e3EELkUaI3dJ7p35ktjj8dSbKTtpCw2CPkNCBy8Vo/NyyGhrOx6YfdMvHw0LygvNOl4srGkvRp4iJZaWKxHt01+Pp9SjaFBxKiXnhzHU/ZKzSz9Xk47Q3dD0hYFEIIIYToAbM+N7QQojgxLZvlvNGPT4Q2hZkjWpuilP4Wn70GaQ6F6Ce8U9uwy7E9nlmtCCmcfGCWaBIzWP8k5ZKw2CO8h2iSnvFhYkJDxokhkhZu1t6MfUbMOWXxwsdmtm3cdkdQtqmxoDxVs99ey9CDS1HnqtiTUvNWb9hj4ROC8oFDhwfl6zeHYcn8PWxT24Ny5nkXM3WZJRj651VawmKPsILCYfZ8l7jdm5XN0puv38n8rk6w2rDlN5lz+s3DW8Jhf9JW/M2MvWr4W8fmKcX4rAf7Dh8alOdPhXbQI852emJyc1DOKkOcMKnn00NIsyha0y8jRAgh2kXz2kAwhlAzuJ3hsrQPf6UXwJnTLz0nYbFHlG4npje1AV3edDEP+0yLKJpTdw/QmKnLTO6v2DXG9te9z/oV369Pm/u4oPy1tZ8PypOTLpVql+2iBwWDYbJPPFwkLIqBYTCEwxAFsB5c6v4ALRwyqQvUvc/6Fd+v528MbRJ9KlX9DuXRLz0pYVH0LTGj6TyiNlIlaE/KRJoUURUx+8A6jDW/WuDp9f05KPixMH90j6C8ZXvoRCfKoep0f2UiYVH0LaMj4YQ2MuyDUWdzQ3uvc5+Kbsp58ZmFeZCFGBTqIAzGkDDYHfyL97rNN/aoJbMMk7AoROX4JdqpzJJtVtMYjQFYs4dTPzzQhRCDhead7tEvfS1hsSZkQw10yCwwOD54l2OC8gOTK4Lydpf2DgAWzNkrKK/f+rugHFvI7sdl4X5s82xEv9NgkjV1GQ6KdbBLHhpeGJS9jaKoBi1Di8LUYcLoN1ZsuSwoT0ysD8o+liUAbNu5KjzGBbDu1FuzjvRjm2cj+p0Gk6yjUP3meu/drLHYPSZr4DjWDhIWB5Q6hpEp+w171/n7B+W1m9a00QYFhxVC9I46apBZsbazjtdcF/pEVpSwOKjUQTjM4DyNsxNIMdZtvqFwExSnsDgZj1SlkixMnuex+m12UsffPS/nfKn11/Ca64DS/QmRhxcyfO4vUU/65dVXCCH6DOuT+VXCYo+YjWr5bIzDYsTyjc5WYmPJawbJuUF5aCgsz3dOQHOc8fuDW24KGxCZ7OaMLgnK4xNhSCPZ6wrRW3wIMVgxW+7C3xeds1rH0B2Y56VC54gYAzPYO6CwkDCoE0aHZIVo79jjAxuHQvbUVBhLctvOe4PyVmfSUNTEYdLFrpw/d9/w+3b8oVB9/YjGqqgzfhm68vHqTJKGhxa49sTCog0GyTJ0OX1N8iwALwWwxswOS7ftAeBbAJYCuAvA8Wa2niQBfAbAsQC2ATjJzK5rVb+ERdE/OCPsmFfhINqJ5V3T6OieQXlicnNQHh4KNYOjI+HETCdMmhMmxye2heWpHe741prMXecvDcqHD4chjy7f+Rl4amlzK2rHbFyhqQKvWSw7GYGfE4aGFgXlPRaGuah3uNA9m7eFIc4GBYOV6Q39VQCfB3BOw7bTAPzYzM4geVpafi+AFwM4OP08DcAX0/+bImFR1Ib4xF+9AFH3h8/Q8ILMNp+5xguLE5MbW5YzWoVOBTU3+W3YentQXrT4pUF5yS5PyVTxwKZrOmuDmBXU7f7sV/zqghfunrLbm4LysBMddjJ8gVyPcHXCOxb+MY4IyhPud/zx9s9GWhwnns6yHpQlK5rZVSSXus3HATgq/ftsAFcgERaPA3COJQaTvyS5mOQ+Zra6Wf0SFkX/0CeGwFWSJ8hNTO5ofUxM+Cu5X7O2qeH3r7FQWB2f3Frq9wshymU97w/Kw04TOeE0kdsnw5i3fhl5zbDbz9kb0qzAMvQSko1v0cvMbFnknL0bBMD7AOyd/r0fgHsajluZbpOwKAYAZ+fihRxvu5cx2gZgzn4u9vYZ25+pv2JD8LxrGnfByL2WoG7csvNHQfnEJa/LHPOV8S8H5Z1j95XahrprkIXoJv5+2GPRE4Py1w97TFB+0uGh8HjbjWHM2/2XhE5tF/32UUH50yt/G5TXTIRlb7OYCd/laeeFN/P8iJ9SNUkGl7YbstbMjogf1uS7zIyceQgSCYuiNkSzp0Q0ZAvnhRPSth33NDmy/e/sNb59U26JOe+YuuF/t6077grKX7k/FAwBYGJyW2ZbmdS9z4ToJv5+2LwjXEZ+4XWXBOUFN+0RlLdNXB/uHwn3bxn7aXi8z6QVS09bhg1zTVemKp6L7p9eXia5D4DpzBWrABzQcNz+6bamRMR1IYQQQghRBVNtfmbIxQBOTP8+EcBFDdvfyISnA9jYyl4RkGZR9JBOlwL9+c+c+8qgfPnEOfCMjbdOCVj35cm6tacM8rQGu84/ICiv33Jrt5pTCXnLaPL4FnXlEQsPCcprt4fLxCNDc4LyjrHQFGa3OeGy9NCQ87aeah3Hse7zcFkk3tDlzAMkv4nEmWUJyZUAPgTgDADnkTwZwN0Ajk8P/z6SsDkrkITOeVOmQoeExR4RDUoa9eTq/5ur0zZ6G8UvPD10nPjAtW/JnHPB+v8IykMMJ71JFxZmcnJTJ00UOXghKU+A9x7d/U6u/WzJ4Um6Tb4dmQ8A78NdxRwZOovpWQX9OLcWxV/jvZt+EZS9HfQOF4vVc8/6H3fUnkHs42aUFWfRzF7bZNfROccagLcVqV/CYk0perPMpptrGn/NP7svzDyyciwr6PkwMf5RVHWOVNEuvRcSSqUGQk/p5NmA0Xvi9z4clpgBgzhea0iZQbmrRsJij6BT4/vsJJk38gwxc1P3hh5R+/cj3qHlvh1hnx08f3HmnE1D4VL1fZPLg/L28QeDsnfGqIOmw9Oxp6DzEqzDNfrx2u8MuZSJAAAXMqhoRqOqPe99EHx/vy2Zd3CmDh9CZZRhXNC1O8PgysNudWBoKCxv3HZHUJ4sOcySv3fmjD4ic8zwkF99CMem14JPOeesus21uatWfg7IZIESVeGTINSVngiLJO8CsBlJXrIJMzuC5GsAfBjAEwAcaWbXpMe+AMm6+xwAYwDebWaX9aLdZRILhJwJHVA4k7L7vppNWGWwfSwM3/B/f//FoOwndSCv370msX7LYFGcMFj4t66hl+CgjddJl5EC6NxGN3p87CXCLxn7ME1OSBp3QtA9bqkSyFtaHyq0P7Zs3SmZa0T4fT4MFQCMRTx1M9/hBOC6rVbkjTsfUGXQ7r/6YtIstsFzzWxtQ/kWAK8E8CV33FoAf25m95I8DMAPkQSP7Gv8JFX62wX9pFg/7VGnzJsTLjtncgznCEEcmue2tO6HvrBXisSfLFxdP1zzLCAr7DnBKpIrnZzryqOuHNY37O6NBXPCNJK7jx4UlO/bflOmzTvH17X8zsmJHW6/jyMatnnEpabsON6mE0ZHRnaPnuIzHsXiuXo76JhTXS0omEpVlIOWoWeAmS0HspOHmTUGcLoVwHySc63PrcN9+qMYg6AtilFUSIkJh5mlfuQ5GrQWFkdGdgvK3gHGB/n2GuFuCFplC/5VL2/ORuEzV5tDr1XzwlwoOGVprfHK1BcxXfGxLTdtD8sbt60IymUEf89o3Sad5t/dX56YNtQL2ENDYZ+OT4RmJ+2ZArS+7lEXY1DjXzTHMMn+EMx7JSwagB+l0cS/1EbKmmleBeC6fhcURT6lC8Q5QpT5iT4iaE2Zs/X0y9ia+KOoj5qQMR/wwl/shbL12PUvEbFl6dhLRyxwchVEA/X7/d72O2NW4vug/BWWqb58PPX/SlM/Is1inGeZ2SqSjwBwKcnfmNlVrU4geSiAjwM4psUxpwA4pdymirLwyzd+Yo89GLK2QK3tOhfOX5qpwz/wfPyvucO7tGyDz2PsH8Cbtt8VlMs2yO8HMr9zP9qBdgO39Dc0FC65DmWWO8Py5JRzkMk4sXktXUHBqw4UFFBHhncNysPD4dL6mFsmr2IsTjlt6NBw+Lv67/THi9nFVJ8I6j0RFs1sVfr/GpLfBnAkgKbCIsn9AXwbwBvN7Pct6l0GYFl6Tg1nvtmNXxbL2GlGNYWRlFDuYbdl+505R7W+MTdnlupiN7I7vmLtSzvBnbNerTHHiILXEPGezvzOfalp6QKun31Mz6mCsVYHkaLXOOEcVCbcCl8vzEL8C2OnzopVoBe4XmHyhm4GyYUAhsxsc/r3MQA+0uL4xQC+B+A0M/tZl5rZc4oG5e4H5s/dOyjvGHsgKEe1cGUYYXuBNCNIFRNgMza2xVtUiDw7NjOnmchorMJz/IPB75+c3BJpgxcGw+/3v/P2naHXeqa9s5WIY1I/3uOiDXrsTJJrjlDzsTeodp8GYMrHJq0pvdAs7g3g2+lDdgTAN8zsEpKvAPA5AHsB+B7JG8zshQDeDuCxAD5I8oNpHceYWR+4mM2cQbkZGhkbd5qTogbyEY1X1uO7jUm4Q0egqsNiDLtltTyD/8ULnxCUfay6N+7x8qB89roLg/Lxu70iKH9/WxgSZYfzBp0/HHqQ3rP+f4PyHBdTcMGiQ4Py2s3XBeXZoNXIe/mbDdfdbfph3iwaT7Mb1K3f6qh9rQotQzfBzO4A8KSc7d9GstTst38MwMe60DRRMVnD72I3Sd0mtG7gl3Dz+mCHi+Hnbd2WbwwFzB3j4fG3bwk1iVsmw/Ak45Ph+ZORZWVf//CQE3D70FNfiIGhD+6/oulv+xWDYRL1e3nIozahc0TIIKrdizp7xBxaZgMfO/g9Qfm007N9aPe5OHD7hplrpo57YVA+74hwGflXD4b9/NuNYXw9s9Bc4PlzXxrWt/nmoDw2EWqQ99nlKUHZB1NvZ1wUvR+K5l6v+v5qJ3RO5pyKA1L345wyiOY53SYvpJi5VR7fz95Jp7BTjg9rlnGCi53e2pmxf393w1SfZMuRsFhT+nfwl4k0j3dvCa/p7e/Mpo378bYw3uSYrQ7KdvItQfm+baFwN+40k5kHgZvoLxr/z+YNRtZRY9XGn7r6ik+OZedK7/ZYaWcZumrtySDcH4NwDb2mnVSaPnlB9pzOHPt6EY+2rsjBRbRkcN6MqsM7cxR1jGjHc7hu+HHxhXevCsrjd2X74A3/FeboXe1y1a4dDpeVfXBmbzsa66P5o6HNYjSrhn+QOAN/n4kk2VhtxqGqNY+ZgNvI5nrP2Lv6bCYlp2AbhDlnNmoWS//dvIMNsvfggrn7BOWxidBUxcefnYqsDmTb7F6UZmm6QYPJwUVEiDwYoincIuFQ6i4U5eHjIm7bsbLl8dm0W34SzBMWQ2Gr7kvdH/vCAUH59Ds/nznGZ6HoFC/o+LG0cdvvCtWXDZwc7+NeB74q+2GVDbgdz1NsJdsyDeQDuORUl3Wk7JeEPC3ggvkHBuXR4flBecv2O4LykIuIUDQjmXgYLUOL1kRCFUQflgM4KY5P+JAtMYHXB3sO9/rsDe3UUTeuWxtq/Sa7EcA3Oraq77Ne2BCW+wXx+rNtGLx7umwKz5OirbHu596pqdYvKnkvP2ImKM6iiOHeiIcY2ojMm7NXUI6l7pqYDHMWZ3IYuze/XmQNGBoK31ZHXLaUP1oUOmLcvP6/grLXAj5/t1OD8jUTlwRln00FAIbMaxLDfhly2smopjGjzewsW4k3JL9q7OKw9jZ+t06X6jIPZL+cnzFWb72/CsGs42XiGjq4+Db5NGCDsIzcKdGA8wP4Et0p7ayevHbPvw7K59z/Ly3r7MeVqzpiMEz2iVZWwmKP8DebXx7dvtMtwcaWofthknSTlLd7uXXjeS1P98vMP9vx30F5x87QsSNvaST2wM1YA8ScDgqmLIzh7Qc3bv1N4TpKX0KNPRh6MPY6vcY6aC4zc0As1/MsFA49WZMG9UmMdkxrLtocxl6VMNg9ytTSkrwLwGYAkwAmzOwIknsA+BaApQDuAnC8ma1vVkczJCzWhKxQ4rQ1GSPYgvuLptargCEXrHk3Z6O4IWMLFwpOoyOhY8Uj5ofBqO9xeV8xWcIbWzRVnju8Q0Ntn01lanJbofNnC9KyCdEe7XjZ7z8nDG+1YevyqpojAqyKoNzPNbO1DeXTAPzYzM4geVpafm/RSlsH+hJCCCGEEKVjSMTFdj4dcByAs9O/zwbw8hbHNkWaxZoSXzoo5vxRByZdPL8Ht9wUlIuGbFm5+eqw/jaCOxeO1xeJhVe2RstrEqUxy6fXQbaLkheEuG6e96I/idlxthPP8/bNPyi9XaIdDDaDuLMtKwR+RNIAfMnMlgHY2+yh4Lv3IUm5XBgJizWhbg+3KujUk3H7eGhmUTi3dAnMht+pH6n/71K/lzcxGJThId6VKAsilwLL0EtIXtNQXpYKg408y8xWkXwEgEtJBkbvZmapIFkYCYs9wnsGey/XmIepp/4PyyxF27xwbughvt6ljWvH6y/ar967ORJQulPvZ08//o51JBYrstvkBuWOPCTqHgM0j6yWq/X9pPHeOUWfFXnp/mKe+R79buVgsCIxKtea2REt6zNblf6/huS3ARwJ4H6S+5jZapL7AFgzk7ZKWKwJ/mb1wmQ8RVNEld2FcCZl4yes9VtuDcrZN+q4Oj/u2Rt5IPfDA7sHoWzqRq+FQ89MPPNnkhaxSqJhawDQBWuO5QCOCcA+tFXpmXy8kOTan2zzwf/dNcKHMdsQlKsei0Xrz2QOarIt2N+Hc0bdTVMSynNwIbkQwJCZbU7/PgbARwBcDOBEAGek/180k/olLJZE0byuPo7iDqclWzB336C80y/B+jA0kSVZc57F/RByIqNZydzs7ibLaDHayBTS4Rt0HSak2APPh2aIBSuvm6BVBr3+nfLGWUaQ8uO1YPq/otcYTXmYSZfpX2jnwuNjp466uKGeCRcP1jPm5r1Yys/CfeDuFR9xAchew5yRMKqDz2byoH+pte6by7QiT8udc1BATJisJTN4HnQdQ5k2i3sD+DaTeWMEwDfM7BKSvwZwHsmTAdwN4PiZVC5hsQ3yJvpsfL7WE7dnwjkyjAzvFpS9JmLCOYcMu0k5+6BpnZu2DlHjM+nO/MPTC4tuqT6WcqodpwIOzXP7w4k9NrFmUgxWrAnJx2sS/Xe2XirPpp0rV+ioA71uU+5YdEL8iBNUvIZqdHRPtz/MAT7H7c9m4Qh/9yG3HOnHxZRbzZhy94YXDAFgyF3n9rG1QXk4Zwk03B/ejyMu3NZYh+Y6oyN7BOW9Fh0WlDfsuCtzztBQOM/sOhK+yM+Hm7sXhnPAhi0uVipb52av2gQpL67fyPDisE3+eeLCkPXHHNH7Z1yc8jK4mNkdAJ6Us30dgKM7rV/CYkXEboYJ57mbsbEaD29Wf4N7g+TMBJCZYOp342T7KHYN/vzYG1kb1+wnxYwNY+v4lJkW9kRj64M7Owo+bIpqyXstiPUDuWPV/S4ZJ4PIfq+R8C+gsSVcZkxbnD2hf+lw98oEsjFAs7aiPl1l63vWCylRjVZBEwvfh5tc8oOxiU2Zc3x2rA3u5Wyz07BuGwvjvcbnZhTa3/H9llN/1sGl3ADxPZkj+mD1zNA/KzkSFtshLzBzxwPRDxBv6xOzO4tpsOLhEsom9h1eu+I1KT5F4ZTTnGTrCydpoo1lZ6cJ9G0YGnL2Se7B4B9mXpOyc/yB8AvdA7nTSTO/j/0DunU/ZJc/Wz/AY3Znvk+9fW1Ge+tTUdZA2OzUti5Tn9ek5DwQ5ow+Iii/YvFbgvJ/r/v3oPzo3Y4JyndtujIoz3cOYBOT21057PeYE9DE5Ea33/VBjgdtNtu1XzuPCCEsNnYz90NmhSUcm7steHRQXjgS9tmq7XdmvnPSzf+bXL/4Ns1xc8qCeY8K63P3x06XeWooo7FtfX/GQob5FZi8F5eYyQHhXiyKpkXtkHbsZTP0gbCY3CH1sk1uBq0vOrQ4iXt4ObJwO8vQZdQZHhB/2NSd+DV2NuEUXa5JzgnHRGbpukOhumrBx0/8QNa7MeNZ75bS/fGZtHMRO7KiqSaz3+e0RTnn91qA7Maymf8t53g75p33hsf77D5++bIG5gBFxN7ORQAAEdFJREFU57WimsGYgFt0v6eKebaoXXSnmv2Z2GEX/c6Z3NNhBbE0qrHz3ZyW0/64l/HEtTHv4qoZGV5gu8x/bFvHbth6c0/bK81iO+R5/XWY1i0aoiWWrq8fiISZ8Q+/ycmi2hsnbDL+huZTDk5NbGhy5PSXdDiplYxvPwDMHQ3tjbbvDJ2lfD/PGw3ttryTwc6xMLJC9q3eOx55A/5w7HrbNm9rl6vp6HGIlWzYms4M/PMeZt7ueNd5BwRlr3HymnSfDrPXAjYQ19B6ez2/QhJ98fAvSy6/fMa5K+Nc4l8we6DViTguxY7Pav2K5W7PFQwjQrz/Xb0j0LhLtWqZi/JCuxvLGY1wtonBbhctxK/wAMDUxIOtK6kBVqI3dNVIWGyHvAmsoHYlW6VztMg4e2Q1SK2O9/XV4sGR0XSExGJLxt7yR0ZCw/LMhJXTB7vODx/ID252YS7cOcNDoSdkJlezGwdD8EJGZ7+L1z55BwAg68iQ56XayHbneR8zYci02H1fTEAYjwjkQ0NZj9mspsIJRhWHUCmqcY5rxLJ9NO4eZms3u4xGrs7Jyaw9Xd2IanNKnpayWvWI53EPXkKyS/HFvjM210czuLRxjUMM+3H+vH3cd4T3x3z3wjnusmtt2n5HUB4dWRKUs/a1kXnS/25TW8Jy5kUqr1+6u3TeLv2yaihhcaaUrFHK3hx+0m39AK+DcOgpbNhdEO8k1E4fbN35QPSYoM6IzWFGw8xyH0a+D71WDsi+WHghPFupH0udPcyiNjcZpyH3MGsne0TVGtzKnYDaiQHah+FJukynYUbqOE9GKTo2Z3CJvl/Hxlu/mHi7S1/2YzmzmpCZVztzAspoqJHXLzW0DbTS0/1VhoRFxO1Y8gZuLEZaLGZZ9gEfvhF77ZA3mvYhKLbtuCfTxlbtqeKtyj9AfWDxTJu80JLRJnkNVtiHR+3y1qB89di3g/L4RNZbc9+FTwnKd41f4r4j/F323fUZQXnN1luC8gJnZ7bvnDBywd07fhmUt22/Oyj738X3WXxZLUc47DAId9SYPHM/tHawiWqMc+4/P/5Hhlo7Q/l+iQr5fqy6WHoZTUfMCSiT6Ses3zuzAMC4D4flszplYqmG2hRPHTQUVWf+8Fpo/7v7/fPnhBotP096ISWvD6vOBhS733y4H7+CEotH68fmsFuRAYD9d3laUD6chwflH2w9JyjvGAvtab0Tz/i4X6FxIZAiTjmFyc3W1doZqi4vZ3UIY9cOcnDpETMRUF0F7vD+GHCd4Jdkfdy5sfEwpluePVLUwSUW4DomNHhzgtgyWUTImA2/azvEvSFb2/h2ux/b0Tx6ATWmEe6HsdBrwWpoaFFQHnF9PD7hg3zH46JW7TgUu6bsHFTMzjPzQppj4uTtZ2OhdGIZxfpDg9t7B5fhobk2b+7+bR27bccdcnDpNdFMBm14QxetI2aTmKdWDynmANMLT8mifRJrk5/QdpkXBsd90Glq8t7YMnlRM8FmQ7uWTNrFWOw697D0GuEx97DKBuT1BvwRz+QcqvbU78mDwD9Aox7ePn5muSGMYsSzD2WzOG13Di0ZCl5D14UcxDXhHbcp4gXrNYmL5uwdlB/cFgsPlGPz2KF9epSIcBh3ECsWizLPSc6HXdrmnOS8zeJkxpSl2DJyN55HdYgOEMOQEwC9psxKYbHoMls7oQaiAzFjpxVO/HPcpLbXgscH5XXbVwRlHzzWG79X/YafR0wr5wUnnyHCC3eTk+Gym3/bfXDL8vD8zLJCVnubFe5i8fViQrt/4w5vfB93MRuDMBb2po34Yh0azMeILZVntXj+wdG5HWfGZMF7V3pPeu+JH3FEypAJyxQLUtz6GvPMPkZdP27NvDjE4lvWLwe4v186fWBnx6ofa2F9O8dDZ6od46ETUcZJqI17p9saXT9HjJvz6i0abqiN+33T1tuDcjY2Y7lmTD1xLKol1hcrBsAsFRYzzOBNsfBAjNzg/m1x/c67grIXDqNOAb0wL8hcY2vnjPiEE7FpjByfH78v8h2Z3UU1UvE2hLtbL2u3Y7xedjaFqGYx6vHae4/TmDF7NMxSUaPzzO8cP99n+ijchh4Lh7kv0SU3IepQ5vZHM5HUQKDOEGlTp33q64s6wKG4plDMHAmLdcZpHeaMhksXsVAiCcVspHyYDK+dme80i8se/xdB+furwu/7352XBeV1/s3Qaay8UXQ8YGlxvA3hkoWHBOWXLHh2UN4xGU5AN4z9ISjftunCoOyXbEdHwviCXhj1oXUAYMItA/t4XZ5RV4cP+eBD2WQdI/yylwvVEwuD4akkm1CI13KPuVA7i+YfFJTnjoTLZNtcPmCfOcTfC1U4W0VNGoa8ScN+Qdlr7bymZYcLBzQ+EWrB/f22cP7STBu27QhTzWUcGVw/9WK1oBVVJCuIfUcmW4qbc7zTkKdTU5gqiH1n1KSpcPKD7EvI8PCuQdmHGNuw5bbWX1FH05W+wNAvMZTl4IIZPqBjRMKFxPbPnfPIoDw27g2zvaNEsQmnCmJLpn5CymYO8blvYx7m/vdtR7PY3fFe9GFUh4dXNiB1QYE2W6Grr/eTY2GP76J2axE7NCC73JjZX0PBptfEPVy9OULrsdePfVpFm/3LUCaGbSZWamSuLWja1Q1hM/uyNdZzB5chjtqIe0lsxvjEGjm4dJuinpUzSQHnl7ni2UbC79xlbui8scFpZyacYwYYcXCJLN+UQsQJwWcO8bG5xrxPQjQ3tH8AR4yuAeSFWKgVVRvTt0EmrdxkZDk0lv3Bj/06vEnHvKnphQp/DbEcxj6YetapYKLTFGkD+qLfCt+vGYcaF14olvGljn1aVFMYNbloQzDzKyyLXFDuDVtbh7rxTjaZfPFuLs6Gmmqd7Wsmv0tWA+vb1HsMNZkP22BWCovxIMT+xytjucXdsBnvzfDmeXBr6LxRdiy9Ssi0KXygbt1+Z+vTC16Dj0vnmcjJeFFco+QnMU+xVFsx+kKwKnhNVZg8dEpWwxTTzLcWDrOZf8KxmTcWY9qYTDagzEO/fnNANCRYUaJzs/9+L0y6zB6Rl+pe3G+xTCPxdJshUbOOnDnNx+3dPrY+c0wjc93yvzfP8UxO+QQKrkkZbalX2MxgrEfn7jpgtYn3GGNWCotCCCGEEL1F3tADRRlv7NF0fl4r5+2Z6qhJdHQae7Io/m0126Ds8kxR78pOU0QV91TufeqnbODwYt6adRybMcq2Jc2uBMxgLHpTlT6YA8pexo3lCIY5O+eMF28bpik9JjsvRiI0+D4u7LmfPd6nTo2l78tE58iLTxns999Z7BpnNNZ9VIaZ+B50BQmLfUPG89FlAcgNUxNN99faZvHVe/5DUF4zHn7HlZs+H5QXzD8wKGcD+DrjXZ8ObQY3W9EHpPfwji1VeI9t73Xr8d8/f25oV/P6PV4flL+8+t8zdSyaF0bL98FnPbFltewSQvg7T02F1xxdIoo8bPOyL8QegEXfXOc7e9kdY2GsyEfuGqZMvH/TdUF5pJ0l2KB93bcjzThKuCUrb2M4FXsYRh6uj9njZZlz7tx4uavDzTN+jvFLqP5h2GE/luJk4Jf+IhmKOo0ReujiE4LyOgvTad674apC9feCwkv3neYSzhGaYmYTWftwN/a8ciPjgd3aSa7jcF5tjM16avAs80JTV/rGG5rkiwB8BsnT+Ctmdkbk+La9oTPntjGQO/VIm+9S/IyOhA+nzVt/676w3JuvG8RCfUT3xzST7vzdFx0alHdOZB1ktu9YFZTraE9XlLJ/+2xw9bmuHD44MgHhC2qUe+Lx3eU2+kgACeEDN5oK0lGHe7zX+LBn/qV5fCIM41RPgaH3+PE+MuJzsXtNuctbXnIw9u7Q+3R/5JBl7GqbYLajZXuLykhF6QvNIpPX/i8AeAGAlQB+TfJiM2sd/GnGX+jfjnPe2L3nb+Re8DfL7vMfHZQXM9TmLN/2+6Ac9VBtp80lE3vb89rUjJNPzJkjek3hg+Gg4SOD8vqRrNbw7jGXDcF7lTuKvtGWLri1s1Qf6afib/FhfT7d2OhImG9363anOYx5a9bBA7XDUDlF+3TenGx4jCmX7SebCrJ18OSql//rGDvPt2nh3DAmqNe8r9/iHTX6YCm/F7B1v253odsyDi2Z6Bzdvce7EfOzOjp/gemGjNQXwiKAIwGsMLM7AIDkuQCOA1CJsNjOck6nb6irN/wkLEdursnJmH1e95fyosFkI1q7TGqwjKYxtmQbCiHXbzwnKPsg4UB2WTj20C87O0ph2pp0y9WWeC3B2PiaoDw+Hsvw0tlSezeI3b/R/QV/563b78psq7v2pW7tAbJtygSLrmFMz37A99Om7XeE+32a0kgqym5Tx7HaHqU5uFQuI9XrF2/OfgDuaSivBPA0fxDJUwCckhZ3AhO3dKFtMyJr0NuLVsyYJQDWRo8qSFEZIhsyIhRydo6tRJ/yUP+21SdRrXa59NdQzaWS8VuUAejHZnStf/t8Hp0JXelbr/X2xBUBZbamMg6MH1I5PwQmvC1FM+aRvKahvMzMlqV/tyUjdUK/CIttkXbcMgAgeU2v7REGFfVttah/q0X9Wy3q3+pQ3w4WZvaiXrehXUqOoFoZqwA0JqvcP90mhBBCCDGbqVxG6hdh8dcADiZ5EMk5AE4AcHGP2ySEEEII0Wsql5H6YhnazCZIvh3AD5G4hZ9lZrdGTlsW2S9mjvq2WtS/1aL+rRb1b3Wob0WGGcpIheibOItCCCGEEKL79MsytBBCCCGE6AESFoUQQgghRFMGTlgk+SKSt5NcQfK0Xrenn4n1JcmTSD5A8ob085ZetHNQIHkWyTUkaxsftF+I9SXJo0hubBi7H+x2GwcJkgeQvJzkbSRvJXlqr9vUr7TTlxq/otsMlM1imvLmt2hIeQPgtZWlBRxg2ulLkicBOMLM3t6TRg4YJJ8NYAuAc8zssF63p5+J9SXJowD8g5m9tNttG0RI7gNgHzO7juQuAK4F8HLNvcVppy81fkW3GTTN4kMpb8xsDMB0yhtRHPVllzGzqwA8GD1QRFFfdhczW21m16V/bwawHElWCVEQ9aWoI4MmLOalvNFNNjPa7ctXkbyJ5PkkD8jZL0RdeQbJG0n+gOShvW7MoEByKYDDAVzd25b0P5G+1PgVXWPQhEXRXf4HwFIzeyKASwGc3eP2CNEu1wE40MyeBOBzAL7T4/YMBCQXAbgAwDvNbFOv29PPRPpS41d0lUETFpUWsDyifWlm68xsZ1r8CoA/6VLbhOgIM9tkZlvSv78PYJTkkh43q68hOYpEuPm6mV3Y6/b0M7G+1PgV3WbQhEWlBSyPaF+mhtjTvAyJbY0QtYfkI0ky/ftIJHPhut62qn9J+/JMAMvN7JO9bk8/005favyKbtMX6f7apRspb2YLzfqS5EcAXGNmFwN4B8mXAZhA4kxwUs8aPACQ/CaAowAsIbkSwIfM7Mzetqo/yetLAKMAYGb/D8CrAfwNyQkA2wGcYIMUGqL7PBPAGwDcTPKGdNv7U62XKEZuXwJ4FKDxK3rDQIXOEUIIIYQQ5TJoy9BCCCGEEKJEJCwKIYQQQoimSFgUQgghhBBNkbAohBBCCCGaImFRCCGEEEI0RcKiEKLvILknyRvSz30kV6V/byH5H71unxBCDBIKnSOE6GtIfhjAFjP7RK/bIoQQg4g0i0KIgYHkUSS/m/79YZJnk/wJybtJvpLkv5K8meQlaUo1kPwTkleSvJbkD11mIiGEmPVIWBRCDDKPAfA8JOkovwbgcjP7YyRZL16SCoyfA/BqM/sTAGcBOL1XjRVCiDoyUOn+hBDC8QMzGyd5M5K0lZek228GsBTAHwE4DMClaardYQCre9BOIYSoLRIWhRCDzE4AMLMpkuMN+XOnkMx/BHCrmT2jVw0UQoi6o2VoIcRs5nYAe5F8BgCQHCV5aI/bJIQQtULCohBi1mJmYwBeDeDjJG8EcAOAP+1tq4QQol4odI4QQgghhGiKNItCCCGEEKIpEhaFEEIIIURTJCwKIYQQQoimSFgUQgghhBBNkbAohBBCCCGaImFRCCGEEEI0RcKiEEIIIYRoyv8H2vswHBhpeQcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njbdu1KUMoSl"
      },
      "source": [
        "#extract features && generate features\n",
        "def extract_features_song(f):\n",
        "    y, _ = librosa.load(f)\n",
        "\n",
        "    # get Mel-frequency cepstral coefficients\n",
        "    mel = librosa.feature.melspectrogram(y, n_fft=1024,\n",
        "        hop_length=512, n_mels=256)\n",
        "   # mel /= np.amax(np.absolute(mel))\n",
        "   # idx = random.randint(0, 1164)\n",
        "    mel = mel[:,:1280]\n",
        "    return mel\n",
        "\n",
        "\n",
        "def generate_features_and_labels():\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "\n",
        "    genres = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
        "    for genre in genres:\n",
        "        sound_files = glob.glob('/content/drive/My Drive/genres/'+genre+'/*.wav')\n",
        "        print('Processing %d songs in %s genre...' % (len(sound_files), genre))\n",
        "        for f in sound_files:\n",
        "            features = extract_features_song(f)\n",
        "            all_features.append(features)\n",
        "            all_labels.append(genre)\n",
        "\n",
        "\n",
        "    # convert labels to one-hot encoding\n",
        "    label_uniq_ids, label_row_ids = np.unique(all_labels, return_inverse=True)\n",
        "    label_row_ids = label_row_ids.astype(np.int32, copy=False)\n",
        "    onehot_labels = to_categorical(label_row_ids, len(label_uniq_ids))\n",
        "    return np.stack(all_features), onehot_labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvTzY8-kMoVb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "c913dcb8-efdd-4cf5-9687-eb0fd1bac07a"
      },
      "source": [
        "#sound_files = glob.glob('/content/drive/My Drive/genres/blues/*.wav')\n",
        "#print(sound_files)\n",
        "features, labels = generate_features_and_labels()\n",
        "print(np.shape(features))\n",
        "print(np.shape(labels))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing 100 songs in blues genre...\n",
            "Processing 100 songs in classical genre...\n",
            "Processing 100 songs in country genre...\n",
            "Processing 100 songs in disco genre...\n",
            "Processing 100 songs in hiphop genre...\n",
            "Processing 100 songs in jazz genre...\n",
            "Processing 100 songs in metal genre...\n",
            "Processing 100 songs in pop genre...\n",
            "Processing 100 songs in reggae genre...\n",
            "Processing 100 songs in rock genre...\n",
            "(1000, 256, 1280)\n",
            "(1000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSPKgTaPMoYs"
      },
      "source": [
        "#divide dataset into test,valid,train\n",
        "features = features.reshape(1000,327680)\n",
        "alldata = np.column_stack((features, labels))\n",
        "\n",
        "np.random.shuffle(alldata[0:100])\n",
        "np.random.shuffle(alldata[100:200])\n",
        "np.random.shuffle(alldata[200:300])\n",
        "np.random.shuffle(alldata[300:400])\n",
        "np.random.shuffle(alldata[400:500])\n",
        "np.random.shuffle(alldata[500:600])\n",
        "np.random.shuffle(alldata[600:700])\n",
        "np.random.shuffle(alldata[700:800])\n",
        "np.random.shuffle(alldata[800:900])\n",
        "np.random.shuffle(alldata[900:1000])\n",
        "\n",
        "test = np.zeros((100,327690))\n",
        "valid = np.zeros((100,327690))\n",
        "train = np.zeros((800,327690))\n",
        "\n",
        "for k in range(0,10):\n",
        "    test[10*k : 10*(k+1)] = alldata[100*k:100*k+10]\n",
        "for m in range(0,10):\n",
        "    valid[10*m: 10*(m+1)] = alldata[100*m+10: 100*m+20]\n",
        "for n in range(0,10):\n",
        "    train[n*80: (n+1)*80] = alldata[100*n+20: 100*(n+1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgXEZClXMoik",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "8f3de677-dbef-4b0a-f505-b29669e5ac58"
      },
      "source": [
        "np.random.shuffle(test)\n",
        "np.random.shuffle(valid)\n",
        "np.random.shuffle(train)\n",
        "\n",
        "#splitidx = int(len(alldata) * training_split)\n",
        "#train, test = alldata[:splitidx,:], alldata[splitidx:,:]\n",
        "\n",
        "print(np.shape(train))\n",
        "print(np.shape(test))\n",
        "print(np.shape(valid))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(800, 327690)\n",
            "(100, 327690)\n",
            "(100, 327690)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRKuFsqcYxs5",
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "9fc596ba-dac8-4e3c-8b12-355530377f1c"
      },
      "source": [
        "\n",
        "#train_empty = train[:,:-10]\n",
        "#train_input = np.zeros((3200,256,128,1))\n",
        "#train_labels = np.zeros((3200,10))\n",
        "#train_empty = train_empty.reshape(800,256,1280,1)\n",
        "'''\n",
        "for i in range(0,800):\n",
        "    train_input[i*4+0] = train_empty[i,:,0*64:(0+2)*64,:]\n",
        "    train_labels[i*4+0] = train[i,-10:]\n",
        "    train_input[i*4+1] = train_empty[i,:,5*64:(5+2)*64,:]\n",
        "    train_labels[i*4+1] = train[i,-10:]\n",
        "    train_input[i*4+2] = train_empty[i,:,10*64:(10+2)*64,:]\n",
        "    train_labels[i*4+2] = train[i,-10:]\n",
        "    train_input[i*4+3] = train_empty[i,:,15*64:(15+2)*64,:]\n",
        "    train_labels[i*4+3] = train[i,-10:]\n",
        "   # train_input[i*19+j] = temp\n",
        "   # train_labels[i*19+j] = train[i,-10:]\n",
        " #       Y_hat = model.predict(temp)\n",
        "  #      count = count + Y_hat\n",
        " #  idx = random.randint(0, 1152)\n",
        " #   empty = train_input[i,:,idx:idx+128,:]\n",
        "   # empty = train_empty[i,:,idx:idx+128,:]\n",
        "   # print(empty.shape)\n",
        "'''\n",
        "#train_input = train[:,:-10]\n",
        "#train_input = train_input.reshape(800,256,1280,1)\n",
        "#train_input = np.log10(1 +  abs(train_input))\n",
        "#train_labels = train[:,-10:]\n",
        "#train_std = np.std(train_input, ddof=1)\n",
        "#train_mean = np.mean(train_input)\n",
        "#train_input = ((train_input - train_mean) / train_std)\n",
        "\n",
        "test_input = test[:,:-10]\n",
        "test_input = test_input.reshape(100,256,1280,1)\n",
        "test_input = np.log(1 +  (test_input))\n",
        "test_labels = test[:,-10:]\n",
        "\n",
        "#test_std = np.std(test_input, ddof=1)\n",
        "#test_mean = np.mean(test_input)\n",
        "#test_input = ((test_input - test_mean) / test_std)\n",
        "\n",
        "valid_input = valid[:,:-10]\n",
        "valid_input = valid_input.reshape(100,256,1280,1)\n",
        "valid_input = np.log(1 +  (valid_input))\n",
        "valid_labels = valid[:,-10:]\n",
        "\n",
        "valid_new = np.zeros((1900,256,128,1))\n",
        "valid_labels_new = np.zeros((1900,10))\n",
        "for i in range(0,100):\n",
        "  for j in range(0,19):\n",
        "    temp = valid_input[i,:,j*64:(j+2)*64,:]\n",
        "    temp = temp.reshape(1, 256, 128, 1)\n",
        "    valid_new[i*19+j] = temp\n",
        "    valid_labels_new[i*19+j] = valid_labels[i]\n",
        "\n",
        "#print(np.shape(train_input))\n",
        "#print(np.shape(train_labels))\n",
        "print(np.shape(test_input))\n",
        "print(np.shape(test_labels))\n",
        "#print(np.shape(valid_input))\n",
        "#print(np.shape(valid_labels))\n",
        "#print(train_input[0])\n",
        "#print(train_labels[1])\n",
        "#print(train_labels[2])\n",
        "#print(train_labels[3])\n",
        "#print(train_labels[0])\n",
        "print(valid_new.shape)\n",
        "print(valid_labels_new.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 256, 1280, 1)\n",
            "(100, 10)\n",
            "(1900, 256, 128, 1)\n",
            "(1900, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0NZMJoWVEOi"
      },
      "source": [
        "for i in range(5):\n",
        "  print(valid_new[i])\n",
        "  print(valid_labels_new[i])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDntEWg0u4dP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "outputId": "bd8cc56a-765d-4917-cab3-5bd861d1cd07"
      },
      "source": [
        "#correspond to net2\n",
        "input_tensor = Input(shape = (256,128,1))\n",
        "x1 = layers.Conv2D(256, (256,4), activation = 'relu', kernel_regularizer = regularizers.l2(0.02))(input_tensor)\n",
        "x2 = layers.Dropout(0.2)(x1)\n",
        "x3 = layers.Conv2D(256, (1,4), activation = 'relu', kernel_regularizer = regularizers.l2(0.01), padding = 'same')(x2)\n",
        "x4 = layers.Conv2D(256, (1,4), activation = 'relu', kernel_regularizer = regularizers.l2(0.01), padding = 'same')(x3)\n",
        "y = layers.add([x2, x3, x4])\n",
        "y1 = layers.MaxPooling2D((1,125))(y)\n",
        "y2 = layers.AveragePooling2D((1,125))(y)\n",
        "concatenated = layers.concatenate([y1, y2], axis = -1)\n",
        "f1 = layers.Flatten()(concatenated)\n",
        "d1 = layers.Dense(128, activation = 'relu')(f1)\n",
        "d2 = layers.Dense(32, activation = 'relu')(d1)\n",
        "d3 = layers.Dropout(0.1)(d2)\n",
        "output_tensor = layers.Dense(10, activation = 'softmax')(d3)\n",
        "\n",
        "model = Model(input_tensor, output_tensor)\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 256, 128, 1)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 1, 125, 256)  262400      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 1, 125, 256)  0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 1, 125, 256)  262400      dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 1, 125, 256)  262400      conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 1, 125, 256)  0           dropout_3[0][0]                  \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 256)    0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 1, 1, 256)    0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 1, 1, 512)    0           max_pooling2d_3[0][0]            \n",
            "                                                                 average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 512)          0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 128)          65664       flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 32)           4128        dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 32)           0           dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 10)           330         dropout_4[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 857,322\n",
            "Trainable params: 857,322\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8msVCG4BYxwZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "outputId": "1336c93b-6c3d-4888-ccc8-26a7f22f0a38"
      },
      "source": [
        "#correspond to net1\n",
        "input_tensor = Input(shape = (256,128,1))\n",
        "x1 = layers.Conv2D(256, (256,4), activation = 'relu', kernel_regularizer = regularizers.l2(0.02))(input_tensor)\n",
        "x2 = layers.Dropout(0.2)(x1)\n",
        "x3 = layers.Conv2D(256, (1,4), activation = 'relu', kernel_regularizer = regularizers.l2(0.01), padding = 'same')(x2)\n",
        "x3_3 = layers.Conv2D(128, (1,4), activation = 'relu', kernel_regularizer = regularizers.l2(0.01), padding = 'same')(x3)\n",
        "x3_4 = layers.Conv2D(128, (1,4), activation = 'relu', kernel_regularizer = regularizers.l2(0.01), padding = 'same')(x3)\n",
        "y1 = layers.MaxPooling2D((1,125))(x2)\n",
        "y2 = layers.AveragePooling2D((1,125))(x3)\n",
        "y3 = layers.MaxPooling2D((1,125))(x3_3)\n",
        "y4 = layers.AveragePooling2D((1,125))(x3_4)\n",
        "concatenated = layers.concatenate([y1, y2, y3, y4], axis = -1)\n",
        "f1 = layers.Flatten()(concatenated)\n",
        "d1 = layers.Dense(256, activation = 'relu')(f1)\n",
        "d2 = layers.Dense(64, activation = 'relu')(d1)\n",
        "d3 = layers.Dropout(0.1)(d2)\n",
        "output_tensor = layers.Dense(10, activation = 'softmax')(d3)\n",
        "\n",
        "model = Model(input_tensor, output_tensor)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 256, 128, 1)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 1, 125, 256)  262400      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 1, 125, 256)  0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 1, 125, 256)  262400      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 1, 125, 128)  131200      conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 1, 125, 128)  131200      conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 256)    0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 1, 1, 256)    0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 128)    0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 1, 1, 128)    0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 1, 1, 768)    0           max_pooling2d_1[0][0]            \n",
            "                                                                 average_pooling2d_1[0][0]        \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "                                                                 average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 768)          0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 256)          196864      flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 64)           16448       dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 64)           0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 10)           650         dropout_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,001,162\n",
            "Trainable params: 1,001,162\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao5bCJKvVD7C"
      },
      "source": [
        "def process_x(song):\n",
        "    song = song.reshape(256,1280,1)\n",
        "    idx = random.randint(0, 1152)\n",
        "    empty = song[:,idx:idx+128,:]\n",
        "\n",
        "\n",
        "    #\n",
        "    return empty\n",
        "\n",
        "#train_input = train[:,:-10]\n",
        "#train_input = train_input.reshape(800,256,1280,1)\n",
        "#train_input = np.log10(1 +  abs(train_input))\n",
        "#train_labels = train[:,-10:]\n",
        "\n",
        "\n",
        "def generate_arrays_from_file():\n",
        "    #x_y \n",
        "\n",
        "    count =1\n",
        "    batch_size = 50\n",
        "    while 1:\n",
        "        batch_x = train[(count - 1) * batch_size:count * batch_size, :-10]\n",
        "        batch_y = train[(count - 1) * batch_size:count * batch_size, -10:]\n",
        "\n",
        "        batch_x = np.array([process_x(song) for song in batch_x])\n",
        "        batch_x = np.log(1 +  (batch_x))\n",
        "\n",
        "       # batch_y = np.array(batch_y).astype(np.float32)\n",
        "       # print(\"count:\"+str(count))\n",
        "        if count<16:\n",
        "          count += 1\n",
        "        else:\n",
        "          count = 1\n",
        "        yield (batch_x, batch_y)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWy1RaRTQ6k4"
      },
      "source": [
        "from keras.callbacks import Callback\n",
        "\n",
        "def evaluate(): # \n",
        "  count = np.array([0,0,0,0,0,0,0,0,0,0])\n",
        "  score = 0\n",
        "  for i in range(0,100):\n",
        "    # print(\"this is i:\", i)\n",
        "      for j in range(0,19):\n",
        "        temp = valid_new[i*19+j,:,:,:]\n",
        "        #print(temp.shape)\n",
        "        temp = temp.reshape(1, 256, 128, 1)\n",
        "        #print(valid_labels_new[i*19+j])\n",
        "        Y_hat = model.predict(temp)\n",
        "        count = count + Y_hat\n",
        "        #print(i,count)\n",
        "      if (np.argmax(count) == np.argmax(valid_labels_new[i*19])):\n",
        "        score += 1\n",
        "      count = np.array([0,0,0,0,0,0,0,0,0,0])\n",
        "\n",
        "  return score # \n",
        "\n",
        "\n",
        "# Callbackacc\n",
        "class Evaluate(Callback):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.accs = []\n",
        "        self.highest = 0.\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        acc = evaluate()\n",
        "        self.accs.append(acc)\n",
        "        if acc >= self.highest: # \n",
        "            self.highest = acc\n",
        "          #  model.save_weights('best_model.weights')\n",
        "\n",
        "        # \n",
        "        print('acc: %s, highest: %s' % (acc, self.highest))\n",
        "\n",
        "\n",
        "evaluator = Evaluate()\n",
        "#print(valid_new.shape)\n",
        "#print(valid_new[0*19+0,:,0*64:(0+2)*64,:].shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7b7VS7xYx0j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bf4f80fd-4a13-4933-c9eb-d2c52ec9206b"
      },
      "source": [
        "#train\n",
        "from keras.callbacks import EarlyStopping\n",
        "#early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
        "#model.fit(X, y, validation_split=0.2, callbacks=[early_stopping])\n",
        "model.compile(optimizer='Adadelta',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#model.fit(train_input, train_labels, epochs=50, batch_size=50,\n",
        " #         validation_split=0.1)\n",
        "\n",
        "'''\n",
        "history = model.fit(train_input,\n",
        "                    train_labels,\n",
        "                    epochs = 50,\n",
        "                    batch_size = 50,\n",
        "                    )\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "model.fit_generator(generate_arrays_from_file(),\n",
        "          steps_per_epoch=16,\n",
        "          epochs=500,\n",
        "          workers=1,\n",
        "          callbacks=[evaluator],\n",
        "          validation_data=(valid_new, valid_labels_new)\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "16/16 [==============================] - 2s 98ms/step - loss: 6.5758 - accuracy: 0.1125 - val_loss: 5.5515 - val_accuracy: 0.1095\n",
            "acc: 10, highest: 10\n",
            "Epoch 2/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 4.9191 - accuracy: 0.1838 - val_loss: 4.6311 - val_accuracy: 0.1242\n",
            "acc: 12, highest: 12\n",
            "Epoch 3/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 3.9199 - accuracy: 0.2138 - val_loss: 3.6505 - val_accuracy: 0.1805\n",
            "acc: 17, highest: 17\n",
            "Epoch 4/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 3.1796 - accuracy: 0.2962 - val_loss: 3.2628 - val_accuracy: 0.1953\n",
            "acc: 21, highest: 21\n",
            "Epoch 5/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 2.7188 - accuracy: 0.3562 - val_loss: 3.0496 - val_accuracy: 0.2074\n",
            "acc: 21, highest: 21\n",
            "Epoch 6/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 2.3877 - accuracy: 0.3887 - val_loss: 2.4052 - val_accuracy: 0.3147\n",
            "acc: 33, highest: 33\n",
            "Epoch 7/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 2.1886 - accuracy: 0.4038 - val_loss: 2.4939 - val_accuracy: 0.2521\n",
            "acc: 26, highest: 33\n",
            "Epoch 8/500\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 1.9961 - accuracy: 0.4350 - val_loss: 2.1496 - val_accuracy: 0.3100\n",
            "acc: 31, highest: 33\n",
            "Epoch 9/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 1.8308 - accuracy: 0.4600 - val_loss: 1.8588 - val_accuracy: 0.4189\n",
            "acc: 46, highest: 46\n",
            "Epoch 10/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 1.8165 - accuracy: 0.4538 - val_loss: 2.4816 - val_accuracy: 0.2116\n",
            "acc: 24, highest: 46\n",
            "Epoch 11/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 1.7079 - accuracy: 0.4913 - val_loss: 1.8891 - val_accuracy: 0.3826\n",
            "acc: 38, highest: 46\n",
            "Epoch 12/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 1.6629 - accuracy: 0.5025 - val_loss: 1.9422 - val_accuracy: 0.3621\n",
            "acc: 39, highest: 46\n",
            "Epoch 13/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 1.5148 - accuracy: 0.5387 - val_loss: 2.1569 - val_accuracy: 0.3289\n",
            "acc: 35, highest: 46\n",
            "Epoch 14/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 1.6121 - accuracy: 0.5537 - val_loss: 1.8605 - val_accuracy: 0.3926\n",
            "acc: 42, highest: 46\n",
            "Epoch 15/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 1.4947 - accuracy: 0.5537 - val_loss: 1.9523 - val_accuracy: 0.3653\n",
            "acc: 38, highest: 46\n",
            "Epoch 16/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 1.4433 - accuracy: 0.5750 - val_loss: 1.6478 - val_accuracy: 0.4521\n",
            "acc: 50, highest: 50\n",
            "Epoch 17/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 1.4483 - accuracy: 0.5713 - val_loss: 1.8047 - val_accuracy: 0.4242\n",
            "acc: 47, highest: 50\n",
            "Epoch 18/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 1.4397 - accuracy: 0.5550 - val_loss: 1.7301 - val_accuracy: 0.4458\n",
            "acc: 49, highest: 50\n",
            "Epoch 19/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 1.4137 - accuracy: 0.5888 - val_loss: 1.5331 - val_accuracy: 0.5000\n",
            "acc: 55, highest: 55\n",
            "Epoch 20/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 1.4170 - accuracy: 0.5813 - val_loss: 1.6266 - val_accuracy: 0.4800\n",
            "acc: 51, highest: 55\n",
            "Epoch 21/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 1.3209 - accuracy: 0.6150 - val_loss: 1.9949 - val_accuracy: 0.3616\n",
            "acc: 42, highest: 55\n",
            "Epoch 22/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 1.3350 - accuracy: 0.5863 - val_loss: 1.5306 - val_accuracy: 0.4953\n",
            "acc: 52, highest: 55\n",
            "Epoch 23/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 1.3301 - accuracy: 0.6150 - val_loss: 1.7843 - val_accuracy: 0.4342\n",
            "acc: 52, highest: 55\n",
            "Epoch 24/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 1.2960 - accuracy: 0.6212 - val_loss: 1.7386 - val_accuracy: 0.4384\n",
            "acc: 49, highest: 55\n",
            "Epoch 25/500\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 1.2834 - accuracy: 0.6200 - val_loss: 1.6447 - val_accuracy: 0.4605\n",
            "acc: 48, highest: 55\n",
            "Epoch 26/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 1.3027 - accuracy: 0.6350 - val_loss: 1.5855 - val_accuracy: 0.4663\n",
            "acc: 50, highest: 55\n",
            "Epoch 27/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 1.2106 - accuracy: 0.6625 - val_loss: 1.5597 - val_accuracy: 0.5179\n",
            "acc: 54, highest: 55\n",
            "Epoch 28/500\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 1.2382 - accuracy: 0.6525 - val_loss: 2.0907 - val_accuracy: 0.3626\n",
            "acc: 39, highest: 55\n",
            "Epoch 29/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 1.2883 - accuracy: 0.6237 - val_loss: 1.5796 - val_accuracy: 0.4926\n",
            "acc: 56, highest: 56\n",
            "Epoch 30/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 1.1576 - accuracy: 0.6812 - val_loss: 2.0535 - val_accuracy: 0.3984\n",
            "acc: 45, highest: 56\n",
            "Epoch 31/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 1.2593 - accuracy: 0.6350 - val_loss: 1.3756 - val_accuracy: 0.5832\n",
            "acc: 69, highest: 69\n",
            "Epoch 32/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 1.1842 - accuracy: 0.6737 - val_loss: 1.5134 - val_accuracy: 0.5216\n",
            "acc: 59, highest: 69\n",
            "Epoch 33/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 1.1439 - accuracy: 0.6737 - val_loss: 1.3743 - val_accuracy: 0.5847\n",
            "acc: 64, highest: 69\n",
            "Epoch 34/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 1.1708 - accuracy: 0.6750 - val_loss: 1.6040 - val_accuracy: 0.4895\n",
            "acc: 53, highest: 69\n",
            "Epoch 35/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 1.1537 - accuracy: 0.6862 - val_loss: 1.4559 - val_accuracy: 0.5437\n",
            "acc: 58, highest: 69\n",
            "Epoch 36/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 1.1853 - accuracy: 0.6637 - val_loss: 1.7045 - val_accuracy: 0.4821\n",
            "acc: 56, highest: 69\n",
            "Epoch 37/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 1.1813 - accuracy: 0.6712 - val_loss: 1.6516 - val_accuracy: 0.4726\n",
            "acc: 52, highest: 69\n",
            "Epoch 38/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 1.1409 - accuracy: 0.6850 - val_loss: 1.4121 - val_accuracy: 0.5563\n",
            "acc: 62, highest: 69\n",
            "Epoch 39/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 1.1180 - accuracy: 0.6650 - val_loss: 1.4897 - val_accuracy: 0.5442\n",
            "acc: 58, highest: 69\n",
            "Epoch 40/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 1.1283 - accuracy: 0.7050 - val_loss: 1.7413 - val_accuracy: 0.4621\n",
            "acc: 50, highest: 69\n",
            "Epoch 41/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 1.1224 - accuracy: 0.6725 - val_loss: 1.5878 - val_accuracy: 0.4963\n",
            "acc: 54, highest: 69\n",
            "Epoch 42/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 1.0835 - accuracy: 0.6988 - val_loss: 1.5773 - val_accuracy: 0.5142\n",
            "acc: 56, highest: 69\n",
            "Epoch 43/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 1.0601 - accuracy: 0.7125 - val_loss: 1.4044 - val_accuracy: 0.5647\n",
            "acc: 61, highest: 69\n",
            "Epoch 44/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 1.0782 - accuracy: 0.7038 - val_loss: 1.4080 - val_accuracy: 0.5611\n",
            "acc: 61, highest: 69\n",
            "Epoch 45/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 1.0916 - accuracy: 0.7100 - val_loss: 1.3927 - val_accuracy: 0.5921\n",
            "acc: 66, highest: 69\n",
            "Epoch 46/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 1.1165 - accuracy: 0.7125 - val_loss: 1.5354 - val_accuracy: 0.5258\n",
            "acc: 54, highest: 69\n",
            "Epoch 47/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 1.0437 - accuracy: 0.7212 - val_loss: 1.3510 - val_accuracy: 0.5968\n",
            "acc: 68, highest: 69\n",
            "Epoch 48/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 1.0753 - accuracy: 0.7188 - val_loss: 1.3910 - val_accuracy: 0.6168\n",
            "acc: 71, highest: 71\n",
            "Epoch 49/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 1.0564 - accuracy: 0.7225 - val_loss: 1.3731 - val_accuracy: 0.5989\n",
            "acc: 66, highest: 71\n",
            "Epoch 50/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 1.0436 - accuracy: 0.7212 - val_loss: 1.3773 - val_accuracy: 0.5979\n",
            "acc: 67, highest: 71\n",
            "Epoch 51/500\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 1.0107 - accuracy: 0.7312 - val_loss: 1.3647 - val_accuracy: 0.6005\n",
            "acc: 68, highest: 71\n",
            "Epoch 52/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 1.0591 - accuracy: 0.7312 - val_loss: 1.7254 - val_accuracy: 0.4863\n",
            "acc: 53, highest: 71\n",
            "Epoch 53/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 1.0777 - accuracy: 0.7113 - val_loss: 1.7249 - val_accuracy: 0.4995\n",
            "acc: 60, highest: 71\n",
            "Epoch 54/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 1.0672 - accuracy: 0.7100 - val_loss: 1.4064 - val_accuracy: 0.5884\n",
            "acc: 68, highest: 71\n",
            "Epoch 55/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 1.0352 - accuracy: 0.7188 - val_loss: 1.5380 - val_accuracy: 0.5384\n",
            "acc: 60, highest: 71\n",
            "Epoch 56/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 1.0051 - accuracy: 0.7575 - val_loss: 1.4258 - val_accuracy: 0.5753\n",
            "acc: 67, highest: 71\n",
            "Epoch 57/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.9902 - accuracy: 0.7475 - val_loss: 1.3906 - val_accuracy: 0.5863\n",
            "acc: 63, highest: 71\n",
            "Epoch 58/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.9477 - accuracy: 0.7550 - val_loss: 1.4418 - val_accuracy: 0.5716\n",
            "acc: 65, highest: 71\n",
            "Epoch 59/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.9644 - accuracy: 0.7450 - val_loss: 1.3477 - val_accuracy: 0.6084\n",
            "acc: 71, highest: 71\n",
            "Epoch 60/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.9820 - accuracy: 0.7500 - val_loss: 1.4262 - val_accuracy: 0.5616\n",
            "acc: 63, highest: 71\n",
            "Epoch 61/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 1.0757 - accuracy: 0.7225 - val_loss: 1.4767 - val_accuracy: 0.5858\n",
            "acc: 67, highest: 71\n",
            "Epoch 62/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.9747 - accuracy: 0.7475 - val_loss: 1.5109 - val_accuracy: 0.5595\n",
            "acc: 63, highest: 71\n",
            "Epoch 63/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.9744 - accuracy: 0.7362 - val_loss: 1.5912 - val_accuracy: 0.5400\n",
            "acc: 58, highest: 71\n",
            "Epoch 64/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.9831 - accuracy: 0.7475 - val_loss: 1.6062 - val_accuracy: 0.5037\n",
            "acc: 57, highest: 71\n",
            "Epoch 65/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.9481 - accuracy: 0.7638 - val_loss: 1.4506 - val_accuracy: 0.5695\n",
            "acc: 60, highest: 71\n",
            "Epoch 66/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.9295 - accuracy: 0.7763 - val_loss: 1.6008 - val_accuracy: 0.5326\n",
            "acc: 54, highest: 71\n",
            "Epoch 67/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 1.0011 - accuracy: 0.7375 - val_loss: 1.3209 - val_accuracy: 0.6089\n",
            "acc: 71, highest: 71\n",
            "Epoch 68/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.9063 - accuracy: 0.7750 - val_loss: 1.5375 - val_accuracy: 0.5332\n",
            "acc: 57, highest: 71\n",
            "Epoch 69/500\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 0.9017 - accuracy: 0.7800 - val_loss: 1.4148 - val_accuracy: 0.5774\n",
            "acc: 62, highest: 71\n",
            "Epoch 70/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.8902 - accuracy: 0.7800 - val_loss: 1.3271 - val_accuracy: 0.6184\n",
            "acc: 65, highest: 71\n",
            "Epoch 71/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.9138 - accuracy: 0.7625 - val_loss: 1.4287 - val_accuracy: 0.5826\n",
            "acc: 63, highest: 71\n",
            "Epoch 72/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.8933 - accuracy: 0.7837 - val_loss: 1.3576 - val_accuracy: 0.6105\n",
            "acc: 66, highest: 71\n",
            "Epoch 73/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.9373 - accuracy: 0.7575 - val_loss: 1.3953 - val_accuracy: 0.5921\n",
            "acc: 66, highest: 71\n",
            "Epoch 74/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.9280 - accuracy: 0.7700 - val_loss: 1.8218 - val_accuracy: 0.4795\n",
            "acc: 52, highest: 71\n",
            "Epoch 75/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.9419 - accuracy: 0.7700 - val_loss: 1.3374 - val_accuracy: 0.6074\n",
            "acc: 68, highest: 71\n",
            "Epoch 76/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.8616 - accuracy: 0.7887 - val_loss: 1.3492 - val_accuracy: 0.6163\n",
            "acc: 69, highest: 71\n",
            "Epoch 77/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.9237 - accuracy: 0.7600 - val_loss: 1.6120 - val_accuracy: 0.5200\n",
            "acc: 54, highest: 71\n",
            "Epoch 78/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.9242 - accuracy: 0.7462 - val_loss: 1.3310 - val_accuracy: 0.6126\n",
            "acc: 67, highest: 71\n",
            "Epoch 79/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.8772 - accuracy: 0.7925 - val_loss: 1.3576 - val_accuracy: 0.6158\n",
            "acc: 67, highest: 71\n",
            "Epoch 80/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.9126 - accuracy: 0.7788 - val_loss: 1.2129 - val_accuracy: 0.6658\n",
            "acc: 75, highest: 75\n",
            "Epoch 81/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.8699 - accuracy: 0.7800 - val_loss: 1.3173 - val_accuracy: 0.6195\n",
            "acc: 68, highest: 75\n",
            "Epoch 82/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.9307 - accuracy: 0.7613 - val_loss: 1.2757 - val_accuracy: 0.6389\n",
            "acc: 71, highest: 75\n",
            "Epoch 83/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.8322 - accuracy: 0.8012 - val_loss: 1.5240 - val_accuracy: 0.5600\n",
            "acc: 62, highest: 75\n",
            "Epoch 84/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.8942 - accuracy: 0.7800 - val_loss: 1.7896 - val_accuracy: 0.4837\n",
            "acc: 54, highest: 75\n",
            "Epoch 85/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.9716 - accuracy: 0.7412 - val_loss: 1.6158 - val_accuracy: 0.5379\n",
            "acc: 62, highest: 75\n",
            "Epoch 86/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.8557 - accuracy: 0.7912 - val_loss: 1.3549 - val_accuracy: 0.6342\n",
            "acc: 68, highest: 75\n",
            "Epoch 87/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.8871 - accuracy: 0.7788 - val_loss: 1.4224 - val_accuracy: 0.5795\n",
            "acc: 64, highest: 75\n",
            "Epoch 88/500\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.8616 - accuracy: 0.8050 - val_loss: 1.3586 - val_accuracy: 0.6037\n",
            "acc: 67, highest: 75\n",
            "Epoch 89/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.8904 - accuracy: 0.7763 - val_loss: 1.3413 - val_accuracy: 0.6268\n",
            "acc: 71, highest: 75\n",
            "Epoch 90/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.8307 - accuracy: 0.7950 - val_loss: 1.4246 - val_accuracy: 0.5947\n",
            "acc: 68, highest: 75\n",
            "Epoch 91/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.8150 - accuracy: 0.8050 - val_loss: 1.2691 - val_accuracy: 0.6184\n",
            "acc: 69, highest: 75\n",
            "Epoch 92/500\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 0.8826 - accuracy: 0.7738 - val_loss: 1.4080 - val_accuracy: 0.5884\n",
            "acc: 67, highest: 75\n",
            "Epoch 93/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.8603 - accuracy: 0.8050 - val_loss: 1.3304 - val_accuracy: 0.6189\n",
            "acc: 66, highest: 75\n",
            "Epoch 94/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.8630 - accuracy: 0.7887 - val_loss: 1.3514 - val_accuracy: 0.6053\n",
            "acc: 67, highest: 75\n",
            "Epoch 95/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.7935 - accuracy: 0.8037 - val_loss: 1.3719 - val_accuracy: 0.6105\n",
            "acc: 65, highest: 75\n",
            "Epoch 96/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.8137 - accuracy: 0.7800 - val_loss: 1.4818 - val_accuracy: 0.5868\n",
            "acc: 66, highest: 75\n",
            "Epoch 97/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.9075 - accuracy: 0.7663 - val_loss: 1.4409 - val_accuracy: 0.5695\n",
            "acc: 68, highest: 75\n",
            "Epoch 98/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.8873 - accuracy: 0.7825 - val_loss: 1.2755 - val_accuracy: 0.6384\n",
            "acc: 74, highest: 75\n",
            "Epoch 99/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.7716 - accuracy: 0.8275 - val_loss: 1.4135 - val_accuracy: 0.6095\n",
            "acc: 67, highest: 75\n",
            "Epoch 100/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.9271 - accuracy: 0.7763 - val_loss: 1.3779 - val_accuracy: 0.6189\n",
            "acc: 73, highest: 75\n",
            "Epoch 101/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.8804 - accuracy: 0.7725 - val_loss: 1.2945 - val_accuracy: 0.6342\n",
            "acc: 71, highest: 75\n",
            "Epoch 102/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.7987 - accuracy: 0.8213 - val_loss: 1.1949 - val_accuracy: 0.6779\n",
            "acc: 77, highest: 77\n",
            "Epoch 103/500\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.8188 - accuracy: 0.8112 - val_loss: 1.2992 - val_accuracy: 0.6279\n",
            "acc: 70, highest: 77\n",
            "Epoch 104/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.8867 - accuracy: 0.7937 - val_loss: 1.4300 - val_accuracy: 0.5916\n",
            "acc: 63, highest: 77\n",
            "Epoch 105/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.8604 - accuracy: 0.7962 - val_loss: 1.1954 - val_accuracy: 0.6921\n",
            "acc: 77, highest: 77\n",
            "Epoch 106/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.8016 - accuracy: 0.8087 - val_loss: 1.1883 - val_accuracy: 0.6816\n",
            "acc: 79, highest: 79\n",
            "Epoch 107/500\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.7662 - accuracy: 0.8300 - val_loss: 1.2748 - val_accuracy: 0.6421\n",
            "acc: 71, highest: 79\n",
            "Epoch 108/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.7260 - accuracy: 0.8363 - val_loss: 1.3537 - val_accuracy: 0.6311\n",
            "acc: 74, highest: 79\n",
            "Epoch 109/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.7887 - accuracy: 0.8250 - val_loss: 1.4503 - val_accuracy: 0.6132\n",
            "acc: 68, highest: 79\n",
            "Epoch 110/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.8055 - accuracy: 0.8138 - val_loss: 1.3135 - val_accuracy: 0.6216\n",
            "acc: 65, highest: 79\n",
            "Epoch 111/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.8290 - accuracy: 0.8062 - val_loss: 1.2723 - val_accuracy: 0.6489\n",
            "acc: 77, highest: 79\n",
            "Epoch 112/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.7944 - accuracy: 0.8100 - val_loss: 1.4164 - val_accuracy: 0.6126\n",
            "acc: 73, highest: 79\n",
            "Epoch 113/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.8468 - accuracy: 0.7950 - val_loss: 1.5655 - val_accuracy: 0.5805\n",
            "acc: 62, highest: 79\n",
            "Epoch 114/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.7137 - accuracy: 0.8500 - val_loss: 1.5271 - val_accuracy: 0.5621\n",
            "acc: 62, highest: 79\n",
            "Epoch 115/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.8989 - accuracy: 0.7875 - val_loss: 1.3410 - val_accuracy: 0.6168\n",
            "acc: 68, highest: 79\n",
            "Epoch 116/500\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 0.7639 - accuracy: 0.8238 - val_loss: 1.5117 - val_accuracy: 0.5858\n",
            "acc: 61, highest: 79\n",
            "Epoch 117/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.7340 - accuracy: 0.8325 - val_loss: 1.3207 - val_accuracy: 0.6384\n",
            "acc: 74, highest: 79\n",
            "Epoch 118/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.8012 - accuracy: 0.8313 - val_loss: 1.4423 - val_accuracy: 0.6053\n",
            "acc: 69, highest: 79\n",
            "Epoch 119/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.8224 - accuracy: 0.8037 - val_loss: 1.4098 - val_accuracy: 0.5905\n",
            "acc: 68, highest: 79\n",
            "Epoch 120/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.7520 - accuracy: 0.8313 - val_loss: 1.1702 - val_accuracy: 0.6884\n",
            "acc: 79, highest: 79\n",
            "Epoch 121/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.8476 - accuracy: 0.7887 - val_loss: 1.2964 - val_accuracy: 0.6416\n",
            "acc: 74, highest: 79\n",
            "Epoch 122/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.7864 - accuracy: 0.8150 - val_loss: 1.2732 - val_accuracy: 0.6611\n",
            "acc: 73, highest: 79\n",
            "Epoch 123/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.7470 - accuracy: 0.8238 - val_loss: 1.6460 - val_accuracy: 0.5689\n",
            "acc: 62, highest: 79\n",
            "Epoch 124/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.7642 - accuracy: 0.8200 - val_loss: 1.2018 - val_accuracy: 0.6926\n",
            "acc: 79, highest: 79\n",
            "Epoch 125/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.7810 - accuracy: 0.8112 - val_loss: 1.5081 - val_accuracy: 0.5737\n",
            "acc: 63, highest: 79\n",
            "Epoch 126/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.6965 - accuracy: 0.8413 - val_loss: 1.3793 - val_accuracy: 0.6174\n",
            "acc: 69, highest: 79\n",
            "Epoch 127/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.8096 - accuracy: 0.8163 - val_loss: 1.3810 - val_accuracy: 0.6037\n",
            "acc: 68, highest: 79\n",
            "Epoch 128/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.7598 - accuracy: 0.8288 - val_loss: 1.2797 - val_accuracy: 0.6542\n",
            "acc: 75, highest: 79\n",
            "Epoch 129/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.7910 - accuracy: 0.8112 - val_loss: 1.3232 - val_accuracy: 0.6368\n",
            "acc: 72, highest: 79\n",
            "Epoch 130/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.7907 - accuracy: 0.8200 - val_loss: 1.2292 - val_accuracy: 0.6563\n",
            "acc: 71, highest: 79\n",
            "Epoch 131/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.7554 - accuracy: 0.8313 - val_loss: 1.4366 - val_accuracy: 0.6268\n",
            "acc: 74, highest: 79\n",
            "Epoch 132/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.8028 - accuracy: 0.8050 - val_loss: 1.5403 - val_accuracy: 0.5763\n",
            "acc: 63, highest: 79\n",
            "Epoch 133/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.7954 - accuracy: 0.8012 - val_loss: 1.2778 - val_accuracy: 0.6479\n",
            "acc: 74, highest: 79\n",
            "Epoch 134/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.7144 - accuracy: 0.8400 - val_loss: 1.2904 - val_accuracy: 0.6516\n",
            "acc: 73, highest: 79\n",
            "Epoch 135/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.7273 - accuracy: 0.8275 - val_loss: 1.1906 - val_accuracy: 0.6779\n",
            "acc: 78, highest: 79\n",
            "Epoch 136/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.7061 - accuracy: 0.8475 - val_loss: 1.2206 - val_accuracy: 0.6684\n",
            "acc: 74, highest: 79\n",
            "Epoch 137/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.7666 - accuracy: 0.8200 - val_loss: 1.3673 - val_accuracy: 0.6232\n",
            "acc: 72, highest: 79\n",
            "Epoch 138/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.7727 - accuracy: 0.8138 - val_loss: 1.3462 - val_accuracy: 0.6337\n",
            "acc: 73, highest: 79\n",
            "Epoch 139/500\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 0.7961 - accuracy: 0.8163 - val_loss: 1.2957 - val_accuracy: 0.6463\n",
            "acc: 67, highest: 79\n",
            "Epoch 140/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6865 - accuracy: 0.8537 - val_loss: 1.3853 - val_accuracy: 0.6037\n",
            "acc: 68, highest: 79\n",
            "Epoch 141/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.7011 - accuracy: 0.8512 - val_loss: 1.3279 - val_accuracy: 0.6332\n",
            "acc: 68, highest: 79\n",
            "Epoch 142/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.7371 - accuracy: 0.8487 - val_loss: 1.2083 - val_accuracy: 0.6879\n",
            "acc: 80, highest: 80\n",
            "Epoch 143/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.7736 - accuracy: 0.8112 - val_loss: 1.2635 - val_accuracy: 0.6500\n",
            "acc: 72, highest: 80\n",
            "Epoch 144/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.7500 - accuracy: 0.8288 - val_loss: 1.2938 - val_accuracy: 0.6453\n",
            "acc: 73, highest: 80\n",
            "Epoch 145/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.7230 - accuracy: 0.8425 - val_loss: 1.2520 - val_accuracy: 0.6621\n",
            "acc: 76, highest: 80\n",
            "Epoch 146/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.8276 - accuracy: 0.8112 - val_loss: 1.2652 - val_accuracy: 0.6437\n",
            "acc: 69, highest: 80\n",
            "Epoch 147/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.7598 - accuracy: 0.8375 - val_loss: 1.2144 - val_accuracy: 0.6726\n",
            "acc: 80, highest: 80\n",
            "Epoch 148/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6693 - accuracy: 0.8550 - val_loss: 1.3867 - val_accuracy: 0.6384\n",
            "acc: 72, highest: 80\n",
            "Epoch 149/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.6923 - accuracy: 0.8425 - val_loss: 1.4132 - val_accuracy: 0.6021\n",
            "acc: 69, highest: 80\n",
            "Epoch 150/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.7407 - accuracy: 0.8300 - val_loss: 1.3376 - val_accuracy: 0.6463\n",
            "acc: 72, highest: 80\n",
            "Epoch 151/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.6782 - accuracy: 0.8512 - val_loss: 1.1636 - val_accuracy: 0.7026\n",
            "acc: 80, highest: 80\n",
            "Epoch 152/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.7816 - accuracy: 0.8025 - val_loss: 1.3857 - val_accuracy: 0.6063\n",
            "acc: 65, highest: 80\n",
            "Epoch 153/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.6385 - accuracy: 0.8612 - val_loss: 1.2807 - val_accuracy: 0.6695\n",
            "acc: 75, highest: 80\n",
            "Epoch 154/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.8997 - accuracy: 0.7800 - val_loss: 1.2328 - val_accuracy: 0.6626\n",
            "acc: 75, highest: 80\n",
            "Epoch 155/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.7239 - accuracy: 0.8475 - val_loss: 1.4915 - val_accuracy: 0.6116\n",
            "acc: 69, highest: 80\n",
            "Epoch 156/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.7041 - accuracy: 0.8487 - val_loss: 1.5009 - val_accuracy: 0.6042\n",
            "acc: 65, highest: 80\n",
            "Epoch 157/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.7456 - accuracy: 0.8375 - val_loss: 1.2571 - val_accuracy: 0.6605\n",
            "acc: 77, highest: 80\n",
            "Epoch 158/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6979 - accuracy: 0.8525 - val_loss: 1.4281 - val_accuracy: 0.5963\n",
            "acc: 63, highest: 80\n",
            "Epoch 159/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.7006 - accuracy: 0.8438 - val_loss: 1.2958 - val_accuracy: 0.6505\n",
            "acc: 72, highest: 80\n",
            "Epoch 160/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.8121 - accuracy: 0.8037 - val_loss: 1.2095 - val_accuracy: 0.6858\n",
            "acc: 79, highest: 80\n",
            "Epoch 161/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6882 - accuracy: 0.8612 - val_loss: 1.1821 - val_accuracy: 0.6800\n",
            "acc: 79, highest: 80\n",
            "Epoch 162/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.6638 - accuracy: 0.8637 - val_loss: 1.3523 - val_accuracy: 0.6337\n",
            "acc: 72, highest: 80\n",
            "Epoch 163/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.6205 - accuracy: 0.8700 - val_loss: 1.2543 - val_accuracy: 0.6679\n",
            "acc: 79, highest: 80\n",
            "Epoch 164/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6927 - accuracy: 0.8462 - val_loss: 1.2992 - val_accuracy: 0.6605\n",
            "acc: 74, highest: 80\n",
            "Epoch 165/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.6924 - accuracy: 0.8525 - val_loss: 1.4376 - val_accuracy: 0.6111\n",
            "acc: 68, highest: 80\n",
            "Epoch 166/500\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 0.8019 - accuracy: 0.8125 - val_loss: 1.2680 - val_accuracy: 0.6711\n",
            "acc: 75, highest: 80\n",
            "Epoch 167/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6859 - accuracy: 0.8475 - val_loss: 1.3381 - val_accuracy: 0.6458\n",
            "acc: 75, highest: 80\n",
            "Epoch 168/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.7131 - accuracy: 0.8288 - val_loss: 1.3208 - val_accuracy: 0.6395\n",
            "acc: 76, highest: 80\n",
            "Epoch 169/500\n",
            "16/16 [==============================] - 1s 56ms/step - loss: 0.6943 - accuracy: 0.8512 - val_loss: 1.3200 - val_accuracy: 0.6605\n",
            "acc: 74, highest: 80\n",
            "Epoch 170/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.7222 - accuracy: 0.8338 - val_loss: 1.1635 - val_accuracy: 0.6926\n",
            "acc: 82, highest: 82\n",
            "Epoch 171/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.6860 - accuracy: 0.8462 - val_loss: 1.6499 - val_accuracy: 0.5647\n",
            "acc: 63, highest: 82\n",
            "Epoch 172/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.6350 - accuracy: 0.8687 - val_loss: 1.2325 - val_accuracy: 0.6805\n",
            "acc: 79, highest: 82\n",
            "Epoch 173/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6513 - accuracy: 0.8562 - val_loss: 1.2002 - val_accuracy: 0.6884\n",
            "acc: 76, highest: 82\n",
            "Epoch 174/500\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 0.6305 - accuracy: 0.8725 - val_loss: 1.2747 - val_accuracy: 0.6705\n",
            "acc: 72, highest: 82\n",
            "Epoch 175/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.7410 - accuracy: 0.8487 - val_loss: 1.3911 - val_accuracy: 0.6158\n",
            "acc: 72, highest: 82\n",
            "Epoch 176/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.7088 - accuracy: 0.8425 - val_loss: 1.1934 - val_accuracy: 0.6784\n",
            "acc: 77, highest: 82\n",
            "Epoch 177/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.7411 - accuracy: 0.8313 - val_loss: 1.6697 - val_accuracy: 0.5379\n",
            "acc: 60, highest: 82\n",
            "Epoch 178/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.7363 - accuracy: 0.8263 - val_loss: 1.4087 - val_accuracy: 0.6589\n",
            "acc: 74, highest: 82\n",
            "Epoch 179/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.7054 - accuracy: 0.8450 - val_loss: 1.5979 - val_accuracy: 0.5663\n",
            "acc: 63, highest: 82\n",
            "Epoch 180/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.7005 - accuracy: 0.8525 - val_loss: 1.2486 - val_accuracy: 0.6705\n",
            "acc: 74, highest: 82\n",
            "Epoch 181/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.7472 - accuracy: 0.8300 - val_loss: 1.6490 - val_accuracy: 0.5689\n",
            "acc: 62, highest: 82\n",
            "Epoch 182/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.6955 - accuracy: 0.8562 - val_loss: 1.2276 - val_accuracy: 0.6832\n",
            "acc: 75, highest: 82\n",
            "Epoch 183/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.7567 - accuracy: 0.8188 - val_loss: 1.2614 - val_accuracy: 0.6637\n",
            "acc: 73, highest: 82\n",
            "Epoch 184/500\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.6960 - accuracy: 0.8388 - val_loss: 1.2911 - val_accuracy: 0.6332\n",
            "acc: 72, highest: 82\n",
            "Epoch 185/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6839 - accuracy: 0.8575 - val_loss: 1.2516 - val_accuracy: 0.6689\n",
            "acc: 75, highest: 82\n",
            "Epoch 186/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.6820 - accuracy: 0.8712 - val_loss: 1.3220 - val_accuracy: 0.6600\n",
            "acc: 74, highest: 82\n",
            "Epoch 187/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.6794 - accuracy: 0.8550 - val_loss: 1.3278 - val_accuracy: 0.6279\n",
            "acc: 73, highest: 82\n",
            "Epoch 188/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.6758 - accuracy: 0.8575 - val_loss: 1.4358 - val_accuracy: 0.6384\n",
            "acc: 73, highest: 82\n",
            "Epoch 189/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.6540 - accuracy: 0.8562 - val_loss: 1.4591 - val_accuracy: 0.6221\n",
            "acc: 71, highest: 82\n",
            "Epoch 190/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.6617 - accuracy: 0.8600 - val_loss: 1.4841 - val_accuracy: 0.5932\n",
            "acc: 61, highest: 82\n",
            "Epoch 191/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.6665 - accuracy: 0.8587 - val_loss: 1.4479 - val_accuracy: 0.6105\n",
            "acc: 72, highest: 82\n",
            "Epoch 192/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.6900 - accuracy: 0.8400 - val_loss: 1.3372 - val_accuracy: 0.6453\n",
            "acc: 69, highest: 82\n",
            "Epoch 193/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6836 - accuracy: 0.8712 - val_loss: 1.3105 - val_accuracy: 0.6553\n",
            "acc: 75, highest: 82\n",
            "Epoch 194/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.7426 - accuracy: 0.8350 - val_loss: 1.2748 - val_accuracy: 0.6563\n",
            "acc: 74, highest: 82\n",
            "Epoch 195/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.7027 - accuracy: 0.8375 - val_loss: 1.3058 - val_accuracy: 0.6811\n",
            "acc: 77, highest: 82\n",
            "Epoch 196/500\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.6668 - accuracy: 0.8500 - val_loss: 1.2740 - val_accuracy: 0.6668\n",
            "acc: 78, highest: 82\n",
            "Epoch 197/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.6555 - accuracy: 0.8637 - val_loss: 1.5283 - val_accuracy: 0.5916\n",
            "acc: 64, highest: 82\n",
            "Epoch 198/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.7334 - accuracy: 0.8475 - val_loss: 1.3577 - val_accuracy: 0.6511\n",
            "acc: 75, highest: 82\n",
            "Epoch 199/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6390 - accuracy: 0.8725 - val_loss: 1.2102 - val_accuracy: 0.6732\n",
            "acc: 78, highest: 82\n",
            "Epoch 200/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.6849 - accuracy: 0.8475 - val_loss: 1.6813 - val_accuracy: 0.5553\n",
            "acc: 60, highest: 82\n",
            "Epoch 201/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.6925 - accuracy: 0.8400 - val_loss: 1.3027 - val_accuracy: 0.6579\n",
            "acc: 75, highest: 82\n",
            "Epoch 202/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.6444 - accuracy: 0.8600 - val_loss: 1.3239 - val_accuracy: 0.6568\n",
            "acc: 73, highest: 82\n",
            "Epoch 203/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5896 - accuracy: 0.8775 - val_loss: 1.2918 - val_accuracy: 0.6816\n",
            "acc: 77, highest: 82\n",
            "Epoch 204/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.7109 - accuracy: 0.8512 - val_loss: 1.3024 - val_accuracy: 0.6674\n",
            "acc: 75, highest: 82\n",
            "Epoch 205/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.6068 - accuracy: 0.8825 - val_loss: 1.6260 - val_accuracy: 0.5732\n",
            "acc: 61, highest: 82\n",
            "Epoch 206/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.7021 - accuracy: 0.8525 - val_loss: 1.2082 - val_accuracy: 0.6853\n",
            "acc: 75, highest: 82\n",
            "Epoch 207/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6118 - accuracy: 0.8800 - val_loss: 1.3051 - val_accuracy: 0.6716\n",
            "acc: 76, highest: 82\n",
            "Epoch 208/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.6887 - accuracy: 0.8425 - val_loss: 1.5607 - val_accuracy: 0.6084\n",
            "acc: 69, highest: 82\n",
            "Epoch 209/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6425 - accuracy: 0.8687 - val_loss: 1.3721 - val_accuracy: 0.6263\n",
            "acc: 71, highest: 82\n",
            "Epoch 210/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.6243 - accuracy: 0.8587 - val_loss: 1.4432 - val_accuracy: 0.6247\n",
            "acc: 70, highest: 82\n",
            "Epoch 211/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6814 - accuracy: 0.8562 - val_loss: 1.3429 - val_accuracy: 0.6337\n",
            "acc: 72, highest: 82\n",
            "Epoch 212/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5979 - accuracy: 0.8750 - val_loss: 1.4948 - val_accuracy: 0.6426\n",
            "acc: 71, highest: 82\n",
            "Epoch 213/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5989 - accuracy: 0.8788 - val_loss: 1.3464 - val_accuracy: 0.6505\n",
            "acc: 77, highest: 82\n",
            "Epoch 214/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5860 - accuracy: 0.8838 - val_loss: 1.3607 - val_accuracy: 0.6411\n",
            "acc: 74, highest: 82\n",
            "Epoch 215/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6296 - accuracy: 0.8700 - val_loss: 1.4051 - val_accuracy: 0.6511\n",
            "acc: 75, highest: 82\n",
            "Epoch 216/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.6168 - accuracy: 0.8838 - val_loss: 1.2016 - val_accuracy: 0.6989\n",
            "acc: 78, highest: 82\n",
            "Epoch 217/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6562 - accuracy: 0.8512 - val_loss: 1.2736 - val_accuracy: 0.6674\n",
            "acc: 76, highest: 82\n",
            "Epoch 218/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.6103 - accuracy: 0.8687 - val_loss: 1.3515 - val_accuracy: 0.6400\n",
            "acc: 74, highest: 82\n",
            "Epoch 219/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.6955 - accuracy: 0.8512 - val_loss: 1.2780 - val_accuracy: 0.6584\n",
            "acc: 73, highest: 82\n",
            "Epoch 220/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.6482 - accuracy: 0.8562 - val_loss: 1.2987 - val_accuracy: 0.6574\n",
            "acc: 73, highest: 82\n",
            "Epoch 221/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.6076 - accuracy: 0.8737 - val_loss: 1.5264 - val_accuracy: 0.6116\n",
            "acc: 66, highest: 82\n",
            "Epoch 222/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.7148 - accuracy: 0.8325 - val_loss: 1.3626 - val_accuracy: 0.6474\n",
            "acc: 76, highest: 82\n",
            "Epoch 223/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.6274 - accuracy: 0.8687 - val_loss: 1.2174 - val_accuracy: 0.6968\n",
            "acc: 78, highest: 82\n",
            "Epoch 224/500\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.5419 - accuracy: 0.8950 - val_loss: 1.2146 - val_accuracy: 0.6932\n",
            "acc: 77, highest: 82\n",
            "Epoch 225/500\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.6158 - accuracy: 0.8737 - val_loss: 1.5019 - val_accuracy: 0.6274\n",
            "acc: 66, highest: 82\n",
            "Epoch 226/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.6144 - accuracy: 0.8825 - val_loss: 1.2767 - val_accuracy: 0.6889\n",
            "acc: 80, highest: 82\n",
            "Epoch 227/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.7740 - accuracy: 0.8225 - val_loss: 1.3194 - val_accuracy: 0.6663\n",
            "acc: 76, highest: 82\n",
            "Epoch 228/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5568 - accuracy: 0.8975 - val_loss: 1.2620 - val_accuracy: 0.6711\n",
            "acc: 72, highest: 82\n",
            "Epoch 229/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.6135 - accuracy: 0.8562 - val_loss: 1.2184 - val_accuracy: 0.6942\n",
            "acc: 78, highest: 82\n",
            "Epoch 230/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.7269 - accuracy: 0.8462 - val_loss: 1.2966 - val_accuracy: 0.6463\n",
            "acc: 72, highest: 82\n",
            "Epoch 231/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.6128 - accuracy: 0.8800 - val_loss: 1.2930 - val_accuracy: 0.6737\n",
            "acc: 79, highest: 82\n",
            "Epoch 232/500\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.6653 - accuracy: 0.8712 - val_loss: 1.3966 - val_accuracy: 0.6579\n",
            "acc: 73, highest: 82\n",
            "Epoch 233/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5749 - accuracy: 0.8950 - val_loss: 1.2653 - val_accuracy: 0.6805\n",
            "acc: 78, highest: 82\n",
            "Epoch 234/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6993 - accuracy: 0.8512 - val_loss: 1.2255 - val_accuracy: 0.6884\n",
            "acc: 78, highest: 82\n",
            "Epoch 235/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5191 - accuracy: 0.9013 - val_loss: 1.2633 - val_accuracy: 0.6658\n",
            "acc: 77, highest: 82\n",
            "Epoch 236/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5823 - accuracy: 0.8913 - val_loss: 1.5015 - val_accuracy: 0.6184\n",
            "acc: 67, highest: 82\n",
            "Epoch 237/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6597 - accuracy: 0.8612 - val_loss: 1.5646 - val_accuracy: 0.6232\n",
            "acc: 73, highest: 82\n",
            "Epoch 238/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.7993 - accuracy: 0.8125 - val_loss: 1.2640 - val_accuracy: 0.6600\n",
            "acc: 76, highest: 82\n",
            "Epoch 239/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.5785 - accuracy: 0.8863 - val_loss: 1.4639 - val_accuracy: 0.6332\n",
            "acc: 72, highest: 82\n",
            "Epoch 240/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5974 - accuracy: 0.8825 - val_loss: 1.2673 - val_accuracy: 0.6663\n",
            "acc: 77, highest: 82\n",
            "Epoch 241/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5749 - accuracy: 0.8925 - val_loss: 1.3953 - val_accuracy: 0.6479\n",
            "acc: 74, highest: 82\n",
            "Epoch 242/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5929 - accuracy: 0.8838 - val_loss: 1.2000 - val_accuracy: 0.6979\n",
            "acc: 79, highest: 82\n",
            "Epoch 243/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6301 - accuracy: 0.8562 - val_loss: 1.3899 - val_accuracy: 0.6458\n",
            "acc: 71, highest: 82\n",
            "Epoch 244/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.6707 - accuracy: 0.8600 - val_loss: 1.3983 - val_accuracy: 0.6195\n",
            "acc: 73, highest: 82\n",
            "Epoch 245/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.5854 - accuracy: 0.8800 - val_loss: 1.3903 - val_accuracy: 0.6332\n",
            "acc: 72, highest: 82\n",
            "Epoch 246/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5441 - accuracy: 0.9000 - val_loss: 1.3836 - val_accuracy: 0.6368\n",
            "acc: 72, highest: 82\n",
            "Epoch 247/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.6127 - accuracy: 0.8687 - val_loss: 1.3401 - val_accuracy: 0.6368\n",
            "acc: 73, highest: 82\n",
            "Epoch 248/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.6449 - accuracy: 0.8587 - val_loss: 1.4696 - val_accuracy: 0.5937\n",
            "acc: 69, highest: 82\n",
            "Epoch 249/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.6326 - accuracy: 0.8700 - val_loss: 1.2896 - val_accuracy: 0.6826\n",
            "acc: 78, highest: 82\n",
            "Epoch 250/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.6960 - accuracy: 0.8400 - val_loss: 1.3031 - val_accuracy: 0.6747\n",
            "acc: 78, highest: 82\n",
            "Epoch 251/500\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 0.6076 - accuracy: 0.8750 - val_loss: 1.2516 - val_accuracy: 0.6711\n",
            "acc: 73, highest: 82\n",
            "Epoch 252/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5807 - accuracy: 0.8888 - val_loss: 1.2702 - val_accuracy: 0.6616\n",
            "acc: 74, highest: 82\n",
            "Epoch 253/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5587 - accuracy: 0.8788 - val_loss: 1.2801 - val_accuracy: 0.6579\n",
            "acc: 75, highest: 82\n",
            "Epoch 254/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.6157 - accuracy: 0.8637 - val_loss: 1.4543 - val_accuracy: 0.6521\n",
            "acc: 74, highest: 82\n",
            "Epoch 255/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.6394 - accuracy: 0.8650 - val_loss: 1.2435 - val_accuracy: 0.6947\n",
            "acc: 81, highest: 82\n",
            "Epoch 256/500\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.6135 - accuracy: 0.8700 - val_loss: 1.2054 - val_accuracy: 0.7089\n",
            "acc: 78, highest: 82\n",
            "Epoch 257/500\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.5759 - accuracy: 0.8825 - val_loss: 1.2074 - val_accuracy: 0.6816\n",
            "acc: 79, highest: 82\n",
            "Epoch 258/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.6403 - accuracy: 0.8800 - val_loss: 1.1423 - val_accuracy: 0.7221\n",
            "acc: 81, highest: 82\n",
            "Epoch 259/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5872 - accuracy: 0.8763 - val_loss: 1.2483 - val_accuracy: 0.6800\n",
            "acc: 75, highest: 82\n",
            "Epoch 260/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.6837 - accuracy: 0.8600 - val_loss: 1.3907 - val_accuracy: 0.6305\n",
            "acc: 76, highest: 82\n",
            "Epoch 261/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6064 - accuracy: 0.8675 - val_loss: 1.3678 - val_accuracy: 0.6763\n",
            "acc: 77, highest: 82\n",
            "Epoch 262/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5065 - accuracy: 0.9100 - val_loss: 1.1234 - val_accuracy: 0.7189\n",
            "acc: 84, highest: 84\n",
            "Epoch 263/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5455 - accuracy: 0.9013 - val_loss: 1.2251 - val_accuracy: 0.6732\n",
            "acc: 78, highest: 84\n",
            "Epoch 264/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.6179 - accuracy: 0.8750 - val_loss: 1.2279 - val_accuracy: 0.6921\n",
            "acc: 79, highest: 84\n",
            "Epoch 265/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5742 - accuracy: 0.8813 - val_loss: 1.3509 - val_accuracy: 0.6621\n",
            "acc: 72, highest: 84\n",
            "Epoch 266/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.6169 - accuracy: 0.8687 - val_loss: 1.2599 - val_accuracy: 0.6816\n",
            "acc: 76, highest: 84\n",
            "Epoch 267/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6486 - accuracy: 0.8612 - val_loss: 1.4050 - val_accuracy: 0.6316\n",
            "acc: 68, highest: 84\n",
            "Epoch 268/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5242 - accuracy: 0.8950 - val_loss: 1.2737 - val_accuracy: 0.6753\n",
            "acc: 77, highest: 84\n",
            "Epoch 269/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5838 - accuracy: 0.8700 - val_loss: 1.3370 - val_accuracy: 0.6453\n",
            "acc: 72, highest: 84\n",
            "Epoch 270/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5413 - accuracy: 0.8913 - val_loss: 1.2480 - val_accuracy: 0.6721\n",
            "acc: 79, highest: 84\n",
            "Epoch 271/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.6053 - accuracy: 0.8737 - val_loss: 1.2183 - val_accuracy: 0.6816\n",
            "acc: 74, highest: 84\n",
            "Epoch 272/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5772 - accuracy: 0.9000 - val_loss: 1.2341 - val_accuracy: 0.6963\n",
            "acc: 78, highest: 84\n",
            "Epoch 273/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.5475 - accuracy: 0.9000 - val_loss: 1.1868 - val_accuracy: 0.6879\n",
            "acc: 79, highest: 84\n",
            "Epoch 274/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.5311 - accuracy: 0.9038 - val_loss: 1.3389 - val_accuracy: 0.6547\n",
            "acc: 75, highest: 84\n",
            "Epoch 275/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.6316 - accuracy: 0.8813 - val_loss: 1.9227 - val_accuracy: 0.5311\n",
            "acc: 59, highest: 84\n",
            "Epoch 276/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6821 - accuracy: 0.8750 - val_loss: 1.4073 - val_accuracy: 0.6395\n",
            "acc: 71, highest: 84\n",
            "Epoch 277/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.5737 - accuracy: 0.8888 - val_loss: 1.3650 - val_accuracy: 0.6421\n",
            "acc: 74, highest: 84\n",
            "Epoch 278/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.6800 - accuracy: 0.8450 - val_loss: 1.7181 - val_accuracy: 0.5479\n",
            "acc: 61, highest: 84\n",
            "Epoch 279/500\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 0.6189 - accuracy: 0.8725 - val_loss: 1.3310 - val_accuracy: 0.6405\n",
            "acc: 75, highest: 84\n",
            "Epoch 280/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6030 - accuracy: 0.8825 - val_loss: 1.1668 - val_accuracy: 0.7026\n",
            "acc: 82, highest: 84\n",
            "Epoch 281/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5875 - accuracy: 0.8838 - val_loss: 1.2099 - val_accuracy: 0.6963\n",
            "acc: 78, highest: 84\n",
            "Epoch 282/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5141 - accuracy: 0.9038 - val_loss: 1.2603 - val_accuracy: 0.6658\n",
            "acc: 74, highest: 84\n",
            "Epoch 283/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5449 - accuracy: 0.9000 - val_loss: 1.2624 - val_accuracy: 0.6800\n",
            "acc: 79, highest: 84\n",
            "Epoch 284/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.5772 - accuracy: 0.8975 - val_loss: 1.4770 - val_accuracy: 0.6484\n",
            "acc: 73, highest: 84\n",
            "Epoch 285/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.6310 - accuracy: 0.8675 - val_loss: 1.3808 - val_accuracy: 0.6226\n",
            "acc: 72, highest: 84\n",
            "Epoch 286/500\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 0.5788 - accuracy: 0.8900 - val_loss: 1.2890 - val_accuracy: 0.6653\n",
            "acc: 75, highest: 84\n",
            "Epoch 287/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5365 - accuracy: 0.8913 - val_loss: 1.3036 - val_accuracy: 0.6616\n",
            "acc: 74, highest: 84\n",
            "Epoch 288/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.5932 - accuracy: 0.8925 - val_loss: 1.4187 - val_accuracy: 0.6237\n",
            "acc: 70, highest: 84\n",
            "Epoch 289/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.6130 - accuracy: 0.8763 - val_loss: 1.2021 - val_accuracy: 0.7000\n",
            "acc: 80, highest: 84\n",
            "Epoch 290/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5435 - accuracy: 0.9112 - val_loss: 1.2380 - val_accuracy: 0.6874\n",
            "acc: 80, highest: 84\n",
            "Epoch 291/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.6419 - accuracy: 0.8562 - val_loss: 1.3173 - val_accuracy: 0.6774\n",
            "acc: 78, highest: 84\n",
            "Epoch 292/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5186 - accuracy: 0.9062 - val_loss: 1.2945 - val_accuracy: 0.6689\n",
            "acc: 77, highest: 84\n",
            "Epoch 293/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.6173 - accuracy: 0.8675 - val_loss: 1.8324 - val_accuracy: 0.5700\n",
            "acc: 61, highest: 84\n",
            "Epoch 294/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5518 - accuracy: 0.8950 - val_loss: 1.2786 - val_accuracy: 0.6732\n",
            "acc: 78, highest: 84\n",
            "Epoch 295/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.6237 - accuracy: 0.8712 - val_loss: 1.3678 - val_accuracy: 0.6595\n",
            "acc: 71, highest: 84\n",
            "Epoch 296/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6636 - accuracy: 0.8662 - val_loss: 1.2838 - val_accuracy: 0.6621\n",
            "acc: 78, highest: 84\n",
            "Epoch 297/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.6842 - accuracy: 0.8612 - val_loss: 1.3112 - val_accuracy: 0.6847\n",
            "acc: 73, highest: 84\n",
            "Epoch 298/500\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 0.4589 - accuracy: 0.9312 - val_loss: 1.3777 - val_accuracy: 0.6653\n",
            "acc: 76, highest: 84\n",
            "Epoch 299/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5386 - accuracy: 0.8988 - val_loss: 1.5125 - val_accuracy: 0.6279\n",
            "acc: 69, highest: 84\n",
            "Epoch 300/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5797 - accuracy: 0.8850 - val_loss: 1.5041 - val_accuracy: 0.6111\n",
            "acc: 67, highest: 84\n",
            "Epoch 301/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5187 - accuracy: 0.9112 - val_loss: 1.2846 - val_accuracy: 0.6674\n",
            "acc: 70, highest: 84\n",
            "Epoch 302/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5506 - accuracy: 0.8988 - val_loss: 1.2233 - val_accuracy: 0.6889\n",
            "acc: 81, highest: 84\n",
            "Epoch 303/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.5066 - accuracy: 0.9137 - val_loss: 1.2670 - val_accuracy: 0.6626\n",
            "acc: 78, highest: 84\n",
            "Epoch 304/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.6459 - accuracy: 0.8637 - val_loss: 1.2901 - val_accuracy: 0.6542\n",
            "acc: 73, highest: 84\n",
            "Epoch 305/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5546 - accuracy: 0.8850 - val_loss: 1.2271 - val_accuracy: 0.6821\n",
            "acc: 73, highest: 84\n",
            "Epoch 306/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5520 - accuracy: 0.8925 - val_loss: 1.1880 - val_accuracy: 0.7058\n",
            "acc: 81, highest: 84\n",
            "Epoch 307/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.6105 - accuracy: 0.8750 - val_loss: 1.2482 - val_accuracy: 0.6626\n",
            "acc: 71, highest: 84\n",
            "Epoch 308/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.6085 - accuracy: 0.8788 - val_loss: 1.3389 - val_accuracy: 0.6711\n",
            "acc: 79, highest: 84\n",
            "Epoch 309/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5350 - accuracy: 0.8938 - val_loss: 1.2418 - val_accuracy: 0.6795\n",
            "acc: 76, highest: 84\n",
            "Epoch 310/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5537 - accuracy: 0.8863 - val_loss: 1.2671 - val_accuracy: 0.6816\n",
            "acc: 75, highest: 84\n",
            "Epoch 311/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.6102 - accuracy: 0.8875 - val_loss: 1.2743 - val_accuracy: 0.6763\n",
            "acc: 78, highest: 84\n",
            "Epoch 312/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5254 - accuracy: 0.9112 - val_loss: 1.1841 - val_accuracy: 0.7037\n",
            "acc: 79, highest: 84\n",
            "Epoch 313/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5329 - accuracy: 0.8963 - val_loss: 1.3105 - val_accuracy: 0.6447\n",
            "acc: 74, highest: 84\n",
            "Epoch 314/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5593 - accuracy: 0.9038 - val_loss: 1.3309 - val_accuracy: 0.6647\n",
            "acc: 78, highest: 84\n",
            "Epoch 315/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5410 - accuracy: 0.8963 - val_loss: 1.1588 - val_accuracy: 0.7053\n",
            "acc: 80, highest: 84\n",
            "Epoch 316/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5487 - accuracy: 0.8938 - val_loss: 1.4350 - val_accuracy: 0.6232\n",
            "acc: 72, highest: 84\n",
            "Epoch 317/500\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 0.5355 - accuracy: 0.9013 - val_loss: 1.4548 - val_accuracy: 0.6137\n",
            "acc: 71, highest: 84\n",
            "Epoch 318/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.5389 - accuracy: 0.8900 - val_loss: 1.6643 - val_accuracy: 0.5726\n",
            "acc: 61, highest: 84\n",
            "Epoch 319/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5543 - accuracy: 0.8988 - val_loss: 1.2982 - val_accuracy: 0.6753\n",
            "acc: 78, highest: 84\n",
            "Epoch 320/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.5535 - accuracy: 0.8925 - val_loss: 1.4020 - val_accuracy: 0.6542\n",
            "acc: 73, highest: 84\n",
            "Epoch 321/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5554 - accuracy: 0.8938 - val_loss: 1.4818 - val_accuracy: 0.6384\n",
            "acc: 73, highest: 84\n",
            "Epoch 322/500\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 0.4922 - accuracy: 0.9087 - val_loss: 1.2443 - val_accuracy: 0.6874\n",
            "acc: 80, highest: 84\n",
            "Epoch 323/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5266 - accuracy: 0.9062 - val_loss: 1.2779 - val_accuracy: 0.7016\n",
            "acc: 81, highest: 84\n",
            "Epoch 324/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5461 - accuracy: 0.8888 - val_loss: 1.4899 - val_accuracy: 0.6516\n",
            "acc: 72, highest: 84\n",
            "Epoch 325/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.7047 - accuracy: 0.8375 - val_loss: 1.4310 - val_accuracy: 0.6326\n",
            "acc: 76, highest: 84\n",
            "Epoch 326/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.6326 - accuracy: 0.8737 - val_loss: 1.6154 - val_accuracy: 0.6047\n",
            "acc: 67, highest: 84\n",
            "Epoch 327/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5364 - accuracy: 0.8988 - val_loss: 1.2579 - val_accuracy: 0.6795\n",
            "acc: 78, highest: 84\n",
            "Epoch 328/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.4915 - accuracy: 0.9162 - val_loss: 1.3287 - val_accuracy: 0.6647\n",
            "acc: 78, highest: 84\n",
            "Epoch 329/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5125 - accuracy: 0.9013 - val_loss: 1.4343 - val_accuracy: 0.6368\n",
            "acc: 72, highest: 84\n",
            "Epoch 330/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6262 - accuracy: 0.8587 - val_loss: 1.5117 - val_accuracy: 0.6142\n",
            "acc: 70, highest: 84\n",
            "Epoch 331/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.5729 - accuracy: 0.8875 - val_loss: 1.3572 - val_accuracy: 0.6700\n",
            "acc: 75, highest: 84\n",
            "Epoch 332/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.7218 - accuracy: 0.8438 - val_loss: 1.2231 - val_accuracy: 0.6911\n",
            "acc: 81, highest: 84\n",
            "Epoch 333/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.4781 - accuracy: 0.9175 - val_loss: 1.3046 - val_accuracy: 0.6774\n",
            "acc: 76, highest: 84\n",
            "Epoch 334/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.4723 - accuracy: 0.9275 - val_loss: 1.3152 - val_accuracy: 0.6653\n",
            "acc: 73, highest: 84\n",
            "Epoch 335/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4582 - accuracy: 0.9225 - val_loss: 1.3195 - val_accuracy: 0.6732\n",
            "acc: 77, highest: 84\n",
            "Epoch 336/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5155 - accuracy: 0.8975 - val_loss: 1.3694 - val_accuracy: 0.6537\n",
            "acc: 72, highest: 84\n",
            "Epoch 337/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.5317 - accuracy: 0.9025 - val_loss: 1.4286 - val_accuracy: 0.6411\n",
            "acc: 69, highest: 84\n",
            "Epoch 338/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.6828 - accuracy: 0.8625 - val_loss: 1.2370 - val_accuracy: 0.6863\n",
            "acc: 79, highest: 84\n",
            "Epoch 339/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.5157 - accuracy: 0.9175 - val_loss: 1.2316 - val_accuracy: 0.6995\n",
            "acc: 79, highest: 84\n",
            "Epoch 340/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5503 - accuracy: 0.9025 - val_loss: 1.5039 - val_accuracy: 0.6168\n",
            "acc: 69, highest: 84\n",
            "Epoch 341/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.5162 - accuracy: 0.9112 - val_loss: 1.2692 - val_accuracy: 0.6979\n",
            "acc: 80, highest: 84\n",
            "Epoch 342/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6169 - accuracy: 0.8662 - val_loss: 1.2323 - val_accuracy: 0.6774\n",
            "acc: 77, highest: 84\n",
            "Epoch 343/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5253 - accuracy: 0.9100 - val_loss: 1.3451 - val_accuracy: 0.6705\n",
            "acc: 78, highest: 84\n",
            "Epoch 344/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5829 - accuracy: 0.8825 - val_loss: 1.3093 - val_accuracy: 0.6979\n",
            "acc: 79, highest: 84\n",
            "Epoch 345/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.4808 - accuracy: 0.9212 - val_loss: 1.4041 - val_accuracy: 0.6479\n",
            "acc: 74, highest: 84\n",
            "Epoch 346/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5340 - accuracy: 0.9050 - val_loss: 1.2180 - val_accuracy: 0.6963\n",
            "acc: 82, highest: 84\n",
            "Epoch 347/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5310 - accuracy: 0.8963 - val_loss: 1.2105 - val_accuracy: 0.6979\n",
            "acc: 78, highest: 84\n",
            "Epoch 348/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.5109 - accuracy: 0.9125 - val_loss: 1.3309 - val_accuracy: 0.6763\n",
            "acc: 77, highest: 84\n",
            "Epoch 349/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6501 - accuracy: 0.8712 - val_loss: 1.5315 - val_accuracy: 0.6347\n",
            "acc: 72, highest: 84\n",
            "Epoch 350/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5615 - accuracy: 0.8963 - val_loss: 1.2114 - val_accuracy: 0.7016\n",
            "acc: 78, highest: 84\n",
            "Epoch 351/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.5418 - accuracy: 0.9025 - val_loss: 1.1868 - val_accuracy: 0.7163\n",
            "acc: 79, highest: 84\n",
            "Epoch 352/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5077 - accuracy: 0.9137 - val_loss: 1.2224 - val_accuracy: 0.6832\n",
            "acc: 79, highest: 84\n",
            "Epoch 353/500\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.4927 - accuracy: 0.9062 - val_loss: 1.2568 - val_accuracy: 0.7026\n",
            "acc: 81, highest: 84\n",
            "Epoch 354/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4867 - accuracy: 0.9137 - val_loss: 1.2643 - val_accuracy: 0.6821\n",
            "acc: 76, highest: 84\n",
            "Epoch 355/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5424 - accuracy: 0.8988 - val_loss: 2.1522 - val_accuracy: 0.5068\n",
            "acc: 56, highest: 84\n",
            "Epoch 356/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5653 - accuracy: 0.9075 - val_loss: 1.2457 - val_accuracy: 0.6953\n",
            "acc: 82, highest: 84\n",
            "Epoch 357/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.5099 - accuracy: 0.9225 - val_loss: 1.2973 - val_accuracy: 0.6816\n",
            "acc: 76, highest: 84\n",
            "Epoch 358/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.4619 - accuracy: 0.9200 - val_loss: 1.1885 - val_accuracy: 0.7105\n",
            "acc: 81, highest: 84\n",
            "Epoch 359/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4614 - accuracy: 0.9175 - val_loss: 1.2672 - val_accuracy: 0.6832\n",
            "acc: 80, highest: 84\n",
            "Epoch 360/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.6325 - accuracy: 0.8737 - val_loss: 1.1633 - val_accuracy: 0.7137\n",
            "acc: 81, highest: 84\n",
            "Epoch 361/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4769 - accuracy: 0.9162 - val_loss: 1.3052 - val_accuracy: 0.6853\n",
            "acc: 79, highest: 84\n",
            "Epoch 362/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6594 - accuracy: 0.8637 - val_loss: 1.1758 - val_accuracy: 0.7258\n",
            "acc: 83, highest: 84\n",
            "Epoch 363/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.5267 - accuracy: 0.9062 - val_loss: 1.2403 - val_accuracy: 0.7047\n",
            "acc: 82, highest: 84\n",
            "Epoch 364/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.5260 - accuracy: 0.8975 - val_loss: 1.2719 - val_accuracy: 0.6779\n",
            "acc: 76, highest: 84\n",
            "Epoch 365/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.4860 - accuracy: 0.9187 - val_loss: 1.2585 - val_accuracy: 0.7037\n",
            "acc: 80, highest: 84\n",
            "Epoch 366/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5342 - accuracy: 0.8988 - val_loss: 1.4226 - val_accuracy: 0.6405\n",
            "acc: 71, highest: 84\n",
            "Epoch 367/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4778 - accuracy: 0.9250 - val_loss: 1.2739 - val_accuracy: 0.6858\n",
            "acc: 77, highest: 84\n",
            "Epoch 368/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.4810 - accuracy: 0.9125 - val_loss: 1.2263 - val_accuracy: 0.6995\n",
            "acc: 82, highest: 84\n",
            "Epoch 369/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5461 - accuracy: 0.8963 - val_loss: 1.3986 - val_accuracy: 0.6484\n",
            "acc: 74, highest: 84\n",
            "Epoch 370/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4920 - accuracy: 0.9162 - val_loss: 1.2494 - val_accuracy: 0.6863\n",
            "acc: 78, highest: 84\n",
            "Epoch 371/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5004 - accuracy: 0.9000 - val_loss: 1.2679 - val_accuracy: 0.6879\n",
            "acc: 81, highest: 84\n",
            "Epoch 372/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.4810 - accuracy: 0.9100 - val_loss: 1.1869 - val_accuracy: 0.6868\n",
            "acc: 79, highest: 84\n",
            "Epoch 373/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.6693 - accuracy: 0.8537 - val_loss: 1.5643 - val_accuracy: 0.6311\n",
            "acc: 72, highest: 84\n",
            "Epoch 374/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5702 - accuracy: 0.8925 - val_loss: 1.2658 - val_accuracy: 0.6847\n",
            "acc: 79, highest: 84\n",
            "Epoch 375/500\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.5414 - accuracy: 0.9100 - val_loss: 1.2204 - val_accuracy: 0.6926\n",
            "acc: 78, highest: 84\n",
            "Epoch 376/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4463 - accuracy: 0.9262 - val_loss: 1.2293 - val_accuracy: 0.6958\n",
            "acc: 79, highest: 84\n",
            "Epoch 377/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4994 - accuracy: 0.9187 - val_loss: 1.5128 - val_accuracy: 0.6237\n",
            "acc: 74, highest: 84\n",
            "Epoch 378/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5044 - accuracy: 0.9075 - val_loss: 1.5549 - val_accuracy: 0.6111\n",
            "acc: 66, highest: 84\n",
            "Epoch 379/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6140 - accuracy: 0.8850 - val_loss: 1.4775 - val_accuracy: 0.6337\n",
            "acc: 68, highest: 84\n",
            "Epoch 380/500\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.4663 - accuracy: 0.9212 - val_loss: 1.2376 - val_accuracy: 0.6942\n",
            "acc: 81, highest: 84\n",
            "Epoch 381/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5206 - accuracy: 0.8975 - val_loss: 1.2462 - val_accuracy: 0.6874\n",
            "acc: 80, highest: 84\n",
            "Epoch 382/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5165 - accuracy: 0.9100 - val_loss: 1.2518 - val_accuracy: 0.6858\n",
            "acc: 77, highest: 84\n",
            "Epoch 383/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6083 - accuracy: 0.8888 - val_loss: 1.2870 - val_accuracy: 0.6889\n",
            "acc: 79, highest: 84\n",
            "Epoch 384/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5182 - accuracy: 0.9087 - val_loss: 1.5005 - val_accuracy: 0.6584\n",
            "acc: 73, highest: 84\n",
            "Epoch 385/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4754 - accuracy: 0.9225 - val_loss: 1.7764 - val_accuracy: 0.5895\n",
            "acc: 68, highest: 84\n",
            "Epoch 386/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.4927 - accuracy: 0.9150 - val_loss: 1.3675 - val_accuracy: 0.6753\n",
            "acc: 77, highest: 84\n",
            "Epoch 387/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5138 - accuracy: 0.8975 - val_loss: 1.3481 - val_accuracy: 0.6663\n",
            "acc: 75, highest: 84\n",
            "Epoch 388/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.5006 - accuracy: 0.9112 - val_loss: 1.6824 - val_accuracy: 0.5758\n",
            "acc: 64, highest: 84\n",
            "Epoch 389/500\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.6336 - accuracy: 0.8737 - val_loss: 1.4193 - val_accuracy: 0.6500\n",
            "acc: 72, highest: 84\n",
            "Epoch 390/500\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 0.5052 - accuracy: 0.9087 - val_loss: 1.2231 - val_accuracy: 0.7079\n",
            "acc: 79, highest: 84\n",
            "Epoch 391/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4865 - accuracy: 0.9075 - val_loss: 1.4255 - val_accuracy: 0.6505\n",
            "acc: 70, highest: 84\n",
            "Epoch 392/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4842 - accuracy: 0.9225 - val_loss: 1.2260 - val_accuracy: 0.7005\n",
            "acc: 81, highest: 84\n",
            "Epoch 393/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4418 - accuracy: 0.9287 - val_loss: 1.2193 - val_accuracy: 0.7053\n",
            "acc: 80, highest: 84\n",
            "Epoch 394/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.4370 - accuracy: 0.9413 - val_loss: 1.2564 - val_accuracy: 0.7126\n",
            "acc: 81, highest: 84\n",
            "Epoch 395/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5052 - accuracy: 0.9075 - val_loss: 1.3926 - val_accuracy: 0.6479\n",
            "acc: 74, highest: 84\n",
            "Epoch 396/500\n",
            "16/16 [==============================] - 1s 56ms/step - loss: 0.4413 - accuracy: 0.9150 - val_loss: 1.3826 - val_accuracy: 0.6642\n",
            "acc: 75, highest: 84\n",
            "Epoch 397/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.6119 - accuracy: 0.8700 - val_loss: 1.2980 - val_accuracy: 0.6505\n",
            "acc: 77, highest: 84\n",
            "Epoch 398/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.5139 - accuracy: 0.8975 - val_loss: 1.2544 - val_accuracy: 0.6900\n",
            "acc: 77, highest: 84\n",
            "Epoch 399/500\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 0.5007 - accuracy: 0.9075 - val_loss: 1.2754 - val_accuracy: 0.6953\n",
            "acc: 79, highest: 84\n",
            "Epoch 400/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5549 - accuracy: 0.8800 - val_loss: 1.2948 - val_accuracy: 0.6774\n",
            "acc: 78, highest: 84\n",
            "Epoch 401/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5019 - accuracy: 0.9100 - val_loss: 1.2136 - val_accuracy: 0.7047\n",
            "acc: 78, highest: 84\n",
            "Epoch 402/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.5036 - accuracy: 0.9125 - val_loss: 1.5504 - val_accuracy: 0.6047\n",
            "acc: 68, highest: 84\n",
            "Epoch 403/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4528 - accuracy: 0.9275 - val_loss: 1.4168 - val_accuracy: 0.6474\n",
            "acc: 76, highest: 84\n",
            "Epoch 404/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6445 - accuracy: 0.8700 - val_loss: 1.1555 - val_accuracy: 0.7295\n",
            "acc: 80, highest: 84\n",
            "Epoch 405/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5308 - accuracy: 0.9000 - val_loss: 1.2760 - val_accuracy: 0.6842\n",
            "acc: 79, highest: 84\n",
            "Epoch 406/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4427 - accuracy: 0.9350 - val_loss: 1.9383 - val_accuracy: 0.6116\n",
            "acc: 65, highest: 84\n",
            "Epoch 407/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5657 - accuracy: 0.9013 - val_loss: 1.1959 - val_accuracy: 0.7042\n",
            "acc: 83, highest: 84\n",
            "Epoch 408/500\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 0.4860 - accuracy: 0.9212 - val_loss: 1.3282 - val_accuracy: 0.6668\n",
            "acc: 75, highest: 84\n",
            "Epoch 409/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4879 - accuracy: 0.9175 - val_loss: 1.4881 - val_accuracy: 0.6489\n",
            "acc: 74, highest: 84\n",
            "Epoch 410/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.4532 - accuracy: 0.9250 - val_loss: 1.3914 - val_accuracy: 0.6547\n",
            "acc: 74, highest: 84\n",
            "Epoch 411/500\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.4861 - accuracy: 0.9175 - val_loss: 1.2102 - val_accuracy: 0.6989\n",
            "acc: 76, highest: 84\n",
            "Epoch 412/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5319 - accuracy: 0.8950 - val_loss: 1.3407 - val_accuracy: 0.6342\n",
            "acc: 73, highest: 84\n",
            "Epoch 413/500\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.4691 - accuracy: 0.9375 - val_loss: 1.1666 - val_accuracy: 0.7042\n",
            "acc: 80, highest: 84\n",
            "Epoch 414/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4676 - accuracy: 0.9100 - val_loss: 1.3331 - val_accuracy: 0.6763\n",
            "acc: 79, highest: 84\n",
            "Epoch 415/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4954 - accuracy: 0.9137 - val_loss: 1.2381 - val_accuracy: 0.7042\n",
            "acc: 81, highest: 84\n",
            "Epoch 416/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4956 - accuracy: 0.9075 - val_loss: 1.4335 - val_accuracy: 0.6384\n",
            "acc: 70, highest: 84\n",
            "Epoch 417/500\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.4712 - accuracy: 0.9212 - val_loss: 1.4253 - val_accuracy: 0.6795\n",
            "acc: 78, highest: 84\n",
            "Epoch 418/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5172 - accuracy: 0.9125 - val_loss: 1.1543 - val_accuracy: 0.7226\n",
            "acc: 82, highest: 84\n",
            "Epoch 419/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.5029 - accuracy: 0.9100 - val_loss: 1.4871 - val_accuracy: 0.6332\n",
            "acc: 73, highest: 84\n",
            "Epoch 420/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4619 - accuracy: 0.9300 - val_loss: 1.3839 - val_accuracy: 0.6611\n",
            "acc: 75, highest: 84\n",
            "Epoch 421/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5045 - accuracy: 0.8963 - val_loss: 1.4943 - val_accuracy: 0.6453\n",
            "acc: 73, highest: 84\n",
            "Epoch 422/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4468 - accuracy: 0.9212 - val_loss: 1.2878 - val_accuracy: 0.6689\n",
            "acc: 78, highest: 84\n",
            "Epoch 423/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4859 - accuracy: 0.9187 - val_loss: 1.8102 - val_accuracy: 0.5395\n",
            "acc: 62, highest: 84\n",
            "Epoch 424/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4878 - accuracy: 0.9125 - val_loss: 1.3362 - val_accuracy: 0.6532\n",
            "acc: 75, highest: 84\n",
            "Epoch 425/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4418 - accuracy: 0.9237 - val_loss: 1.3224 - val_accuracy: 0.6779\n",
            "acc: 77, highest: 84\n",
            "Epoch 426/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.5530 - accuracy: 0.8938 - val_loss: 1.3893 - val_accuracy: 0.6737\n",
            "acc: 77, highest: 84\n",
            "Epoch 427/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4998 - accuracy: 0.9087 - val_loss: 1.5872 - val_accuracy: 0.6305\n",
            "acc: 70, highest: 84\n",
            "Epoch 428/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6183 - accuracy: 0.8800 - val_loss: 1.4697 - val_accuracy: 0.6421\n",
            "acc: 74, highest: 84\n",
            "Epoch 429/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4046 - accuracy: 0.9350 - val_loss: 1.3865 - val_accuracy: 0.6732\n",
            "acc: 77, highest: 84\n",
            "Epoch 430/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4426 - accuracy: 0.9287 - val_loss: 1.2802 - val_accuracy: 0.7111\n",
            "acc: 82, highest: 84\n",
            "Epoch 431/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4538 - accuracy: 0.9250 - val_loss: 1.2879 - val_accuracy: 0.6905\n",
            "acc: 81, highest: 84\n",
            "Epoch 432/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4653 - accuracy: 0.9212 - val_loss: 1.4857 - val_accuracy: 0.6542\n",
            "acc: 75, highest: 84\n",
            "Epoch 433/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.6575 - accuracy: 0.8637 - val_loss: 1.9798 - val_accuracy: 0.5132\n",
            "acc: 52, highest: 84\n",
            "Epoch 434/500\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 0.5160 - accuracy: 0.9087 - val_loss: 1.3640 - val_accuracy: 0.6737\n",
            "acc: 75, highest: 84\n",
            "Epoch 435/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4841 - accuracy: 0.9250 - val_loss: 1.4399 - val_accuracy: 0.6826\n",
            "acc: 79, highest: 84\n",
            "Epoch 436/500\n",
            "16/16 [==============================] - 1s 55ms/step - loss: 0.4653 - accuracy: 0.9200 - val_loss: 1.3111 - val_accuracy: 0.6958\n",
            "acc: 79, highest: 84\n",
            "Epoch 437/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5045 - accuracy: 0.9112 - val_loss: 1.2858 - val_accuracy: 0.6884\n",
            "acc: 79, highest: 84\n",
            "Epoch 438/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4805 - accuracy: 0.9162 - val_loss: 1.5270 - val_accuracy: 0.6447\n",
            "acc: 73, highest: 84\n",
            "Epoch 439/500\n",
            "16/16 [==============================] - 1s 56ms/step - loss: 0.4261 - accuracy: 0.9312 - val_loss: 1.4618 - val_accuracy: 0.6526\n",
            "acc: 75, highest: 84\n",
            "Epoch 440/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.5817 - accuracy: 0.8813 - val_loss: 1.3227 - val_accuracy: 0.6842\n",
            "acc: 78, highest: 84\n",
            "Epoch 441/500\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.4616 - accuracy: 0.9250 - val_loss: 1.5769 - val_accuracy: 0.6026\n",
            "acc: 74, highest: 84\n",
            "Epoch 442/500\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.4717 - accuracy: 0.9237 - val_loss: 1.2865 - val_accuracy: 0.6889\n",
            "acc: 74, highest: 84\n",
            "Epoch 443/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.5080 - accuracy: 0.9000 - val_loss: 1.1921 - val_accuracy: 0.7116\n",
            "acc: 78, highest: 84\n",
            "Epoch 444/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.4710 - accuracy: 0.9200 - val_loss: 1.3472 - val_accuracy: 0.6868\n",
            "acc: 77, highest: 84\n",
            "Epoch 445/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5280 - accuracy: 0.9112 - val_loss: 1.3881 - val_accuracy: 0.6316\n",
            "acc: 72, highest: 84\n",
            "Epoch 446/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5472 - accuracy: 0.8850 - val_loss: 1.3191 - val_accuracy: 0.6853\n",
            "acc: 77, highest: 84\n",
            "Epoch 447/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4960 - accuracy: 0.9187 - val_loss: 1.2170 - val_accuracy: 0.6942\n",
            "acc: 80, highest: 84\n",
            "Epoch 448/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.4156 - accuracy: 0.9350 - val_loss: 1.2776 - val_accuracy: 0.6753\n",
            "acc: 81, highest: 84\n",
            "Epoch 449/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.5309 - accuracy: 0.8925 - val_loss: 1.3384 - val_accuracy: 0.6684\n",
            "acc: 77, highest: 84\n",
            "Epoch 450/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.6138 - accuracy: 0.8775 - val_loss: 1.3438 - val_accuracy: 0.6679\n",
            "acc: 75, highest: 84\n",
            "Epoch 451/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.4749 - accuracy: 0.9175 - val_loss: 1.3841 - val_accuracy: 0.6684\n",
            "acc: 76, highest: 84\n",
            "Epoch 452/500\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.5034 - accuracy: 0.9150 - val_loss: 1.6423 - val_accuracy: 0.6068\n",
            "acc: 72, highest: 84\n",
            "Epoch 453/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4446 - accuracy: 0.9362 - val_loss: 1.3496 - val_accuracy: 0.6911\n",
            "acc: 77, highest: 84\n",
            "Epoch 454/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4818 - accuracy: 0.9112 - val_loss: 1.2460 - val_accuracy: 0.6858\n",
            "acc: 79, highest: 84\n",
            "Epoch 455/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4870 - accuracy: 0.9050 - val_loss: 1.5074 - val_accuracy: 0.6521\n",
            "acc: 76, highest: 84\n",
            "Epoch 456/500\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.5952 - accuracy: 0.8888 - val_loss: 1.3081 - val_accuracy: 0.6779\n",
            "acc: 78, highest: 84\n",
            "Epoch 457/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5090 - accuracy: 0.9100 - val_loss: 1.2672 - val_accuracy: 0.6942\n",
            "acc: 81, highest: 84\n",
            "Epoch 458/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4740 - accuracy: 0.9175 - val_loss: 1.2562 - val_accuracy: 0.6963\n",
            "acc: 78, highest: 84\n",
            "Epoch 459/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4446 - accuracy: 0.9275 - val_loss: 1.3190 - val_accuracy: 0.6926\n",
            "acc: 78, highest: 84\n",
            "Epoch 460/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4998 - accuracy: 0.9162 - val_loss: 1.3914 - val_accuracy: 0.6753\n",
            "acc: 74, highest: 84\n",
            "Epoch 461/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5327 - accuracy: 0.9062 - val_loss: 1.4444 - val_accuracy: 0.6305\n",
            "acc: 72, highest: 84\n",
            "Epoch 462/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5272 - accuracy: 0.8925 - val_loss: 1.3937 - val_accuracy: 0.6563\n",
            "acc: 79, highest: 84\n",
            "Epoch 463/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4602 - accuracy: 0.9225 - val_loss: 1.4698 - val_accuracy: 0.6563\n",
            "acc: 71, highest: 84\n",
            "Epoch 464/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4831 - accuracy: 0.9087 - val_loss: 1.6711 - val_accuracy: 0.6026\n",
            "acc: 69, highest: 84\n",
            "Epoch 465/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5008 - accuracy: 0.9150 - val_loss: 1.3069 - val_accuracy: 0.6947\n",
            "acc: 78, highest: 84\n",
            "Epoch 466/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4460 - accuracy: 0.9362 - val_loss: 1.6370 - val_accuracy: 0.6168\n",
            "acc: 70, highest: 84\n",
            "Epoch 467/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5064 - accuracy: 0.9013 - val_loss: 1.2773 - val_accuracy: 0.6858\n",
            "acc: 79, highest: 84\n",
            "Epoch 468/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5353 - accuracy: 0.8863 - val_loss: 1.3620 - val_accuracy: 0.6684\n",
            "acc: 76, highest: 84\n",
            "Epoch 469/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5501 - accuracy: 0.9025 - val_loss: 1.2546 - val_accuracy: 0.6905\n",
            "acc: 75, highest: 84\n",
            "Epoch 470/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4275 - accuracy: 0.9362 - val_loss: 1.3149 - val_accuracy: 0.6837\n",
            "acc: 79, highest: 84\n",
            "Epoch 471/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5100 - accuracy: 0.9112 - val_loss: 1.5062 - val_accuracy: 0.6516\n",
            "acc: 70, highest: 84\n",
            "Epoch 472/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4825 - accuracy: 0.9125 - val_loss: 1.1483 - val_accuracy: 0.7032\n",
            "acc: 81, highest: 84\n",
            "Epoch 473/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4532 - accuracy: 0.9312 - val_loss: 1.3420 - val_accuracy: 0.6805\n",
            "acc: 78, highest: 84\n",
            "Epoch 474/500\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.5746 - accuracy: 0.8988 - val_loss: 1.3083 - val_accuracy: 0.6642\n",
            "acc: 73, highest: 84\n",
            "Epoch 475/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5282 - accuracy: 0.8950 - val_loss: 1.1735 - val_accuracy: 0.7158\n",
            "acc: 83, highest: 84\n",
            "Epoch 476/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.5214 - accuracy: 0.9025 - val_loss: 1.3661 - val_accuracy: 0.6695\n",
            "acc: 76, highest: 84\n",
            "Epoch 477/500\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.4411 - accuracy: 0.9362 - val_loss: 1.3379 - val_accuracy: 0.6800\n",
            "acc: 79, highest: 84\n",
            "Epoch 478/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.4464 - accuracy: 0.9300 - val_loss: 1.3619 - val_accuracy: 0.6689\n",
            "acc: 77, highest: 84\n",
            "Epoch 479/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4454 - accuracy: 0.9325 - val_loss: 1.3229 - val_accuracy: 0.6926\n",
            "acc: 80, highest: 84\n",
            "Epoch 480/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4925 - accuracy: 0.9137 - val_loss: 1.2721 - val_accuracy: 0.6937\n",
            "acc: 82, highest: 84\n",
            "Epoch 481/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.4429 - accuracy: 0.9287 - val_loss: 1.5253 - val_accuracy: 0.6242\n",
            "acc: 73, highest: 84\n",
            "Epoch 482/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5281 - accuracy: 0.9050 - val_loss: 1.3629 - val_accuracy: 0.6789\n",
            "acc: 77, highest: 84\n",
            "Epoch 483/500\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.4324 - accuracy: 0.9300 - val_loss: 1.4332 - val_accuracy: 0.6532\n",
            "acc: 74, highest: 84\n",
            "Epoch 484/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5008 - accuracy: 0.9038 - val_loss: 1.2305 - val_accuracy: 0.6911\n",
            "acc: 78, highest: 84\n",
            "Epoch 485/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4305 - accuracy: 0.9312 - val_loss: 1.2527 - val_accuracy: 0.6889\n",
            "acc: 76, highest: 84\n",
            "Epoch 486/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4356 - accuracy: 0.9262 - val_loss: 1.8253 - val_accuracy: 0.6126\n",
            "acc: 68, highest: 84\n",
            "Epoch 487/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5841 - accuracy: 0.8775 - val_loss: 1.5066 - val_accuracy: 0.6421\n",
            "acc: 71, highest: 84\n",
            "Epoch 488/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4051 - accuracy: 0.9362 - val_loss: 1.3351 - val_accuracy: 0.6763\n",
            "acc: 79, highest: 84\n",
            "Epoch 489/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.4991 - accuracy: 0.9112 - val_loss: 1.2300 - val_accuracy: 0.7000\n",
            "acc: 79, highest: 84\n",
            "Epoch 490/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4706 - accuracy: 0.9187 - val_loss: 1.3018 - val_accuracy: 0.7084\n",
            "acc: 75, highest: 84\n",
            "Epoch 491/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5026 - accuracy: 0.9050 - val_loss: 1.5854 - val_accuracy: 0.6079\n",
            "acc: 68, highest: 84\n",
            "Epoch 492/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.4924 - accuracy: 0.9125 - val_loss: 1.2055 - val_accuracy: 0.7042\n",
            "acc: 80, highest: 84\n",
            "Epoch 493/500\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.4319 - accuracy: 0.9388 - val_loss: 1.3551 - val_accuracy: 0.6984\n",
            "acc: 80, highest: 84\n",
            "Epoch 494/500\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.4397 - accuracy: 0.9162 - val_loss: 1.4317 - val_accuracy: 0.6584\n",
            "acc: 75, highest: 84\n",
            "Epoch 495/500\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.4548 - accuracy: 0.9287 - val_loss: 1.3224 - val_accuracy: 0.6832\n",
            "acc: 75, highest: 84\n",
            "Epoch 496/500\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.6008 - accuracy: 0.8875 - val_loss: 1.2971 - val_accuracy: 0.7026\n",
            "acc: 79, highest: 84\n",
            "Epoch 497/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4563 - accuracy: 0.9187 - val_loss: 1.4150 - val_accuracy: 0.6663\n",
            "acc: 76, highest: 84\n",
            "Epoch 498/500\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.5056 - accuracy: 0.9137 - val_loss: 1.3675 - val_accuracy: 0.6858\n",
            "acc: 77, highest: 84\n",
            "Epoch 499/500\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5015 - accuracy: 0.9075 - val_loss: 1.2409 - val_accuracy: 0.6921\n",
            "acc: 78, highest: 84\n",
            "Epoch 500/500\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5123 - accuracy: 0.8988 - val_loss: 1.3702 - val_accuracy: 0.6779\n",
            "acc: 77, highest: 84\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fc1901c0828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbkvsvCMYx3Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0c81df2b-cc34-4b2c-b129-bf7d1a306e95"
      },
      "source": [
        "#test on the test dataset\n",
        "count = np.array([0,0,0,0,0,0,0,0,0,0])\n",
        "score = 0\n",
        "for i in range(0,100):\n",
        "   # print(\"this is i:\", i)\n",
        "    for j in range(0,19):\n",
        "      temp = test_input[i,:,j*64:(j+2)*64,:]\n",
        "      temp = temp.reshape(1, 256, 128, 1)\n",
        "      Y_hat = model.predict(temp)\n",
        "      count = count + Y_hat\n",
        "    if (np.argmax(count) == np.argmax(test_labels[i])):\n",
        "        score += 1\n",
        "    count = np.array([0,0,0,0,0,0,0,0,0,0])\n",
        "\n",
        "print(score)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "69\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}